{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu2yKfM6F+9RFMLFCzBL6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanhVanLe0605/Data-Mining-For-Business-Analytics-In-Python/blob/main/Chapter_20_Text_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we introduce text as a form of data.\n",
        "- First, we discuss a tabular representation of text data in which each column is a word, each row is a document, and each cell is a 0 or 1, indicating whether that column's word is present in that row's document.\n",
        "- Then we consider how to move from unstructured documents to this structured matrix.\n",
        "- Finnally, we illustrate how to integrate this process into the standard data mining procedures covered in earlier parts of the book."
      ],
      "metadata": {
        "id": "DnzhDww6s_uR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.1. [INTRODUCTION](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=_tud5FVMy6dV&line=1&uniqifier=1)\n",
        "\n",
        "20.2.[THE TABULAR REPRESENTATION OF TEXT: TERM-DOCUMENT MATRIX AND \"BAG-OF-WORDS\"](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=tlry-74ezIfL&line=1&uniqifier=1)\n",
        "\n",
        "20.3. [BAG-OF-WORDS VS. MEANING EXTRACTION AT DOCUMENT LEVEL](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=yqryZDJ1zdYq&line=1&uniqifier=1)\n",
        "\n",
        "20.4. [PREPROCESSING THE TEXT](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=Ulj1YLIWzvzW&line=1&uniqifier=1)\n",
        "\n",
        "20.5. [IMPLEMENTING DATA MINING METHODS](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=PcvJBFVQ0y9z&line=1&uniqifier=1)\n",
        "\n",
        "20.6. [EXAMPLE: ONLINE DISCUSSION ON AUTOS AND ELECTRONICS](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=eKYVcdku05FR&line=1&uniqifier=1)\n",
        "\n",
        "20.7. [SUMMARY](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=Xb7hf_1o1FRS&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "MI9tAHHry2IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python**\n",
        "\n",
        "In this chapter, we will **pandas** for data handling and **scikit-learn** for the feature creation and model building. The Natural Language Toolkit will be used for more advanced text processing (nltk: https://www.nltk.org)."
      ],
      "metadata": {
        "id": "FBwxRhsVuEOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required functionality for this chapter\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Install smba library as it's not found\n",
        "!pip install dmba\n",
        "\n",
        "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
        "\n",
        "# download data required for NLTK\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXd6y3s6wK0v",
        "outputId": "6389a17c-c63f-4cf3-8807-70838f9ce9a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dmba\n",
            "  Downloading dmba-0.2.4-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from dmba) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from dmba) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dmba) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dmba) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dmba) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from dmba) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->dmba) (1.17.0)\n",
            "Downloading dmba-0.2.4-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dmba\n",
            "Successfully installed dmba-0.2.4\n",
            "Colab environment detected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.1. INTRODUCTION"
      ],
      "metadata": {
        "id": "_tud5FVMy6dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Traditional Data Types**\n",
        "Up to this point, data mining has primarily focused on structured data types:\n",
        "\n",
        "* a. **Numerical**\n",
        "* b. **Binary** (yes/no)\n",
        "\n",
        "* c. **Multicategory**\n",
        "\n",
        "**2. The Role of Text in Predictive Analytics**\n",
        "In many modern applications, data exists in **text form**. In these scenarios:\n",
        "\n",
        "* a. The **predictor variables (features)** are embedded directly within the text of documents.\n",
        "* b. **Automated algorithms** are required to process this unstructured data.\n",
        "\n",
        "**3. Real-world Classification Examples**\n",
        "* a. **Internet Service Providers (ISP):** Using algorithms to **classify** support tickets as *urgent* or *routine* for efficient routing.\n",
        "\n",
        "* b. **Legal Industry:** Automating the discovery process by classifying massive volumes of documents as *relevant* or *irrelevant*.\n",
        "\n",
        "**4. Drivers of Text Mining Growth**\n",
        "The field has expanded significantly due to the availability of **social media data** (Twitter feeds, blogs, online forums, news).\n",
        "\n",
        "* a. **Adoption Rate:** According to the *Rexer Analytics 2013 Data Mining Survey*, 25% to 40% of data miners utilize text data.\n",
        "\n",
        "* b. **Research Impact:** High public availability of this data provides a repository for researchers to hone and improve text mining methods.\n",
        "\n",
        "**5. Emerging Application Areas**\n",
        "\n",
        "* a. A key area of growth is applying text mining methods to **notes and transcripts** from contact centers and service centers."
      ],
      "metadata": {
        "id": "JeqkPg614eD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.2. THE TABULAR REPRESENTATION OF TEXT: TERM-DOCUMENT MATRIX AND \"BAG-OF-WORDS\""
      ],
      "metadata": {
        "id": "tlry-74ezIfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. The Term-Document Matrix**\n",
        "To analyze text quantitatively, unstructured sentences must be converted into a structured matrix format:\n",
        "* **Documents:** The distinct units of text (e.g., sentences $S1, S2, S3$).\n",
        "* **Terms:** The individual words extracted from these documents.\n",
        "* **Structure:** A matrix where rows represent terms and columns represent documents (or vice versa). The cell values represent the **frequency** of the term in that document.\n",
        "\n",
        "**2. Implementation in Python**\n",
        "* **Library:** `scikit-learn` (a standard machine learning library).\n",
        "* **Tool:** `CountVectorizer`.\n",
        "    * *Process:* Collect documents into a list $\\rightarrow$ Apply `CountVectorizer` to generate the matrix.\n",
        "\n",
        "**3. The \"Bag-of-Words\" (BoW) Approach**\n",
        "This matrix representation relies on the **Bag-of-Words** assumption:\n",
        "* **Ignored:** Grammar, syntax, and word order do not matter.\n",
        "* **Preserved:** Only the presence and frequency of words matter.\n",
        "* **Result:** Text is transformed into **tabular data** (numerical features) suitable for standard algorithms.\n",
        "\n",
        "\n",
        "\n",
        "**4. Applications and Challenges**\n",
        "* **Utility:** Once in tabular form, the data can be used for **Clustering**, **Classification**, or **Prediction** (by appending an outcome variable).\n",
        "* **Reality Check:** Text mining is rarely simple.\n",
        "    * Raw data usually requires significant **preprocessing** (cleaning).\n",
        "    * **Human review** is often indispensable to validate the relevance of documents before training."
      ],
      "metadata": {
        "id": "pw9Jr9Xj6yOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.1. TERM-DOCUMENT MATRIX REPRESENTATION OF WORDS IN SENTENCES S1-S3**"
      ],
      "metadata": {
        "id": "B-hKKEok7u3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    'this is the first sentence.',\n",
        "    'this is a second sentence.',\n",
        "    'the third sentence is here.'\n",
        "]\n",
        "# learn features based on text\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiIyxt7i66fQ",
        "outputId": "39a8d31e-4a58-4ac5-e6fb-570b0f89d841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          S1  S2  S3\n",
            "first      1   0   0\n",
            "here       0   0   1\n",
            "is         1   1   1\n",
            "second     0   1   0\n",
            "sentence   1   1   1\n",
            "the        1   0   1\n",
            "third      0   0   1\n",
            "this       1   1   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.3. BAG-OF-WORDS VS. MEANING EXTRACTION AT DOCUMENT LEVEL"
      ],
      "metadata": {
        "id": "yqryZDJ1zdYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Two Distinct Goals in Text Mining**\n",
        "We can categorize text mining tasks into two levels of complexity:\n",
        "* **Goal A: Classification & Clustering (The focus of this book)**\n",
        "    * *Task:* Labeling a document as belonging to a specific class or grouping similar documents together.\n",
        "    * *Method:* Uses standard statistical and machine learning predictive models (similar to those used for numerical data).\n",
        "    * *Requirement:* A sizable collection of documents (**Corpus**) and pre-labeled data for training.\n",
        "* **Goal B: Extracting Detailed Meaning**\n",
        "    * *Task:* Deriving deep understanding from a single document.\n",
        "    * *Method:* Requires complex algorithms to handle grammar, syntax, punctuation, and natural language logic.\n",
        "    * *Field:* This is the domain of **Natural Language Processing (NLP)**.\n",
        "\n",
        "**2. The Challenge of \"Meaning\"**\n",
        "Extracting meaning is far more formidable than probabilistic classification due to:\n",
        "* **Ambiguity:** Identical words can have vastly different meanings depending on context.\n",
        "* **Context Sensitivity:**\n",
        "    * *Example:* \"Hitchcock **shot** The Birds in Bodega Bay.\"\n",
        "    * *Interpretation 1:* Filming a movie (Correct in film context).\n",
        "    * *Interpretation 2:* Hunting birds (Possible interpretation if context is ignored).\n",
        "* **Resolution:** Humans use cultural and social context to resolve ambiguity; computers struggle with this in a simple Bag-of-Words model.\n",
        "\n",
        "\n",
        "\n",
        "**3. Scope of Analysis**\n",
        "* **In Scope:** We will focus on **probabilistic assignment**—using word frequencies to predict which class a document belongs to (e.g., \"Urgent\" vs. \"Routine\").\n",
        "* **Out of Scope:** We will not attempt to program the computer to \"understand\" the documents in the human sense (deep NLP)."
      ],
      "metadata": {
        "id": "gsStR4H5-PmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.4. PREPROCESSING THE TEXT"
      ],
      "metadata": {
        "id": "Ulj1YLIWzvzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Theory vs. Reality**\n",
        "* **Simple Theory:** Previous examples used clean text where words were separated by single spaces and sentences ended with periods. Simple rules could easily parse this.\n",
        "* **Complex Reality:** Real-world text data is \"messy.\" Preparing text for mining is significantly more involved than preparing numerical or categorical data.\n",
        "\n",
        "**2. Common Data Issues (The \"Dirty\" Examples)**\n",
        "Consider the modified sentences ($S1, S2, S3, S4$) presented in the text. They introduce typical noise found in raw data:\n",
        "\n",
        "  a. **Extra Spaces:** Irregular spacing between words.\n",
        "\n",
        "  b. **Non-alpha Characters:** Usage of punctuation and emoticons (e.g., `!!`, `:)`, `,`).\n",
        "\n",
        "  c. **Inconsistent Capitalization:** Random upper/lower case usage (e.g., \"Sentence\" capitalized in the middle).\n",
        "\n",
        "  d. **Misspellings:** Typographical errors (e.g., \"forth\" instead of \"fourth\").\n",
        "\n",
        "**3. The Concept of a \"Corpus\"**\n",
        "* **Definition:** A *corpus* refers to a large, fixed, and standard collection of documents.\n",
        "* **Purpose:**\n",
        "    a. Used to train text preprocessing algorithms.\n",
        "    \n",
        "    b. Serves as a benchmark to compare the results of different algorithms.\n",
        "* **Example:** The **Brown Corpus** (compiled at Brown University in the 1960s), which contains 500 English documents across various types."
      ],
      "metadata": {
        "id": "t61yApmzAOfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.2. TERM-DOCUMENT MATRIX REPRESENTATION OF WORDS IN SENTENCES S1-S4**"
      ],
      "metadata": {
        "id": "actXD2s5-ach"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "\n",
        "# Learn features based on text. Special characters are excluded in the analysis\n",
        "count_vext = CountVectorizer()\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu175tC9-deq",
        "outputId": "aebd0ab4-419c-41cf-ff4a-3e4a072b3ac0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           S1  S2  S3  S4\n",
            "all         0   0   0   1\n",
            "first       1   0   0   0\n",
            "forth       0   0   0   1\n",
            "here        0   0   1   0\n",
            "is          1   1   1   0\n",
            "of          0   0   0   1\n",
            "second      0   1   0   0\n",
            "sentence    1   1   1   0\n",
            "sentences   0   0   0   1\n",
            "the         1   0   1   0\n",
            "third       0   0   1   0\n",
            "this        1   1   0   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "Mia-1iZPz9w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization (The First Step)**\n",
        "* **Definition:** The automated process of dividing raw text into separate units called \"tokens\" (or terms).\n",
        "* **Delimiters:** Software uses characters like spaces, commas, or colons to decide where one token ends and another begins.\n",
        "* **Result:** A raw list of terms that forms the basis of the Term-Document Matrix.\n",
        "\n",
        "**2. The Problem: \"Bulk and Noise\"**\n",
        "Creating a matrix from every single token leads to significant issues:\n",
        "* **Bulk (High Dimensionality):** The English language has over a million words. Including every date stamp, email address, or typo creates a massive, computationally heavy matrix.\n",
        "* **Noise (Irrelevant Data):** Many terms do not help in prediction and confuse the model.\n",
        "    * *Examples:* Boilerplate text (email signatures), random punctuation, or overly common words.\n",
        "\n",
        "\n",
        "\n",
        "**3. Text Reduction Strategies**\n",
        "To combat \"Bulk and Noise,\" preprocessing is essential:\n",
        "* **Stopwords Removal:** Eliminating common words (e.g., \"and\", \"the\", \"is\") that carry little semantic meaning for classification.\n",
        "* **Filtering Boilerplate:** Removing repetitive standard text (like legal disclaimers in emails) that appears in every document but adds no unique information.\n",
        "* **Goal:** To reduce the number of variables (columns) to only those that aid in analysis, making the model faster and more accurate."
      ],
      "metadata": {
        "id": "6Vyau79aBopW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.3 TOKENIZATION OF S1-S4 EXAMPLE**"
      ],
      "metadata": {
        "id": "Qh-WtkeFBzm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for term frequency of second example\n",
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "# learn features based on text. Include special characters that are part of a word in the analysis\n",
        "count_vect = CountVectorizer(token_pattern = ' [a-zA-Z!:]+')\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6rdtjilB5mK",
        "outputId": "2f22f0a8-247d-4a99-a43f-d20806866e5d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            S1  S2  S3  S4\n",
            ":            0   1   0   0\n",
            "a            0   1   0   0\n",
            "all          0   0   0   1\n",
            "first        1   0   0   0\n",
            "here         0   0   1   0\n",
            "is           1   1   1   0\n",
            "of           0   0   0   1\n",
            "second       0   1   0   0\n",
            "sentence     0   1   1   0\n",
            "sentence!!   1   0   0   0\n",
            "sentences    0   0   0   1\n",
            "the          1   0   0   0\n",
            "third        0   0   1   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Reduction**"
      ],
      "metadata": {
        "id": "ZLZaz5qm0BOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effective text reduction focuses on removing noise and reducing the vocabulary size to improve model performance. Below are the key techniques:\n",
        "\n",
        "### 1. Stopword Removal\n",
        "  a. **Tools:** Most software, such as the `CountVectorizer` class in `scikit-learn`, includes generic stopword lists for removing frequently occurring terms.\n",
        "  b. **Customization:** Users can review the extensive default list in `scikit-learn` or provide a custom list using the `stop_words` argument.\n",
        "\n",
        "### 2. Vocabulary Reduction Strategies\n",
        "Additional techniques to reduce text volume and focus on meaningful content include:\n",
        "\n",
        "* **Stemming**\n",
        "\n",
        "    a. A linguistic method that reduces different variants of words to a common core (root form).\n",
        "\n",
        "* **Frequency Filters**\n",
        "\n",
        "    a. Eliminate terms occurring in the great majority of documents (stop-words-like behavior).\n",
        "\n",
        "    b. Eliminate very rare terms to reduce noise.\n",
        "\n",
        "    c. Limit the vocabulary to the top *n* most frequent terms.\n",
        "\n",
        "* **Synonyms & Formatting**\n",
        "\n",
        "    a. Consolidate synonyms or synonymous phrases.\n",
        "\n",
        "    b. Ignore letter case (usually converting all text to lowercase).\n",
        "\n",
        "* **Normalization**\n",
        "\n",
        "    a. Replace specific terms within a category with the general category name.\n",
        "    \n",
        "    b. *Example:* Replacing distinct e-mail addresses with `emailtoken` or different numbers with `numbertoken`."
      ],
      "metadata": {
        "id": "x1VU0qFUDssM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table 20.5 presents the text reduction step applied to the four sentences example, after tokenization. We can see the number of terms has been reduced to five"
      ],
      "metadata": {
        "id": "RybMPwAQD2HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Presence/Absence vs. Frequency**"
      ],
      "metadata": {
        "id": "aS3EBYxS0FHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Bag-of-Words\" model can be implemented in two ways:\n",
        "\n",
        "a. **Frequency-Based (Count)**\n",
        "   * Counts how many times a term appears.\n",
        "   * **Use Case:** When repetition implies intensity (e.g., repeated mentions of \"IP address\" in a support ticket indicate a specific technical issue).\n",
        "\n",
        "b. **Presence/Absence (Binary)**\n",
        "   * Records only if a term exists (1) or not (0), ignoring counts.\n",
        "   * **Use Case:** Classification tasks where the mere existence of a term is a key predictor (e.g., a specific vendor name in forensic accounting).\n",
        "   * **Implementation:** Set `binary=True` in `CountVectorizer`."
      ],
      "metadata": {
        "id": "u4l2Z9RhT060"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Frequency-Inverse Document Frequency (TF_IDF)**"
      ],
      "metadata": {
        "id": "RziuGZXL0J9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
        "\n",
        "a. **The Concept**\n",
        "   * It highlights terms that are frequent in a specific document but rare across the entire corpus.\n",
        "\n",
        "b. **The Formula**\n",
        "   * **Term Frequency ($TF$):** Count of term $t$ in document $d$.\n",
        "   * **Inverse Document Frequency ($IDF$):**\n",
        "     $$IDF(t) = 1 + \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing } t}\\right)$$\n",
        "   * **Final Score:**\n",
        "     $$TF\\text{-}IDF = TF(t,d) \\times IDF(t)$$\n",
        "\n",
        "c. **Interpretation**\n",
        "   * **High score:** Rare term appearing frequently in the document.\n",
        "   * **Low score:** Term appearing in almost all documents (stopwords-like behavior) or absent terms."
      ],
      "metadata": {
        "id": "8aljJ_-AUDfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 20.5. Text redunction of S1-S4 (After tokenization)**"
      ],
      "metadata": {
        "id": "gOPMpBvi0Yte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 20.6. TF-IDF MATRIX FOR S1-S4 EXAMPLE (AFTER TOKENIZATION AND TEXT REDUCTION)**"
      ],
      "metadata": {
        "id": "Tnyz41wv0hNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Terms to Concepts: Latent Semantic Indexing**"
      ],
      "metadata": {
        "id": "8L3a9E1E0qRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction: Latent Semantic Indexing (LSI)\n",
        "LSI (or LSA) reduces the complexity of text data by transforming \"Terms\" into \"Concepts\".\n",
        "\n",
        "a. **Mechanism**\n",
        "   * Similar to PCA (Principal Component Analysis), it groups correlated terms into linear combinations.\n",
        "   * **Example:** Terms like *alternator, battery, headlights* $\\rightarrow$ mapped to concept **\"Alternator Failure\"**.\n",
        "\n",
        "b. **Trade-off**\n",
        "   * **Pros:** Handles synonyms and reduces noise; improves manageability for modeling.\n",
        "   * **Cons:** Creates a **\"Blackbox\"** model. The resulting concepts may not always have a clear human-readable meaning, but they effectively cluster related documents."
      ],
      "metadata": {
        "id": "SLX7x1dLUPjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.5. IMPLEMENTING DATA MINING METHODS"
      ],
      "metadata": {
        "id": "PcvJBFVQ0y9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once text is converted into a numeric matrix, standard Data Mining methods are applied:\n",
        "\n",
        "a. **Clustering:** Grouping similar documents (e.g., clustering medical reports by symptoms).\n",
        "\n",
        "b. **Prediction:** Predicting continuous values (e.g., time to resolve a ticket).\n",
        "\n",
        "c. **Classification (Labeling):** Assigning categories to documents (the most common application)."
      ],
      "metadata": {
        "id": "NXOpQ9eFURB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.6. EXAMPLE: ONLINE DISCUSSION ON AUTOS AND ELECTRONICS"
      ],
      "metadata": {
        "id": "eKYVcdku05FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.7. SUMMARY"
      ],
      "metadata": {
        "id": "Xb7hf_1o1FRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distinction between NLP and Text Mining**\n",
        "\n",
        "- **Natural Language Processing (NLP):** Focuses on extracting meaning from a single document.\n",
        "- **Text Mining:** Focuses on classifying or labeling numerous documents in a probabilistic fashion.\n",
        "- *Note:* This chapter concentrates on Text Mining.\n",
        "\n",
        "**Preprocessing Challenges**\n",
        "\n",
        "- Preprocessing text is more varied and involved than preparing numerical data.\n",
        "- The ultimate goal is to produce a matrix where rows represent **terms** and columns represent **documents**.\n",
        "\n",
        "**Dimensionality Reduction**\n",
        "\n",
        "- **Vocabulary Reduction:** Necessary because the sheer number of terms in natural language is excessive for effective model-building.\n",
        "\n",
        "- **Concept Extraction:** A final major reduction involves using a limited set of *concepts* instead of raw terms.\n",
        "\n",
        "- **Analogy:** This captures variation in documents similarly to how *Principal Components* capture variation in numerical data.\n",
        "\n",
        "**Final Output & Application**\n",
        "\n",
        "- The process results in a **quantitative matrix** where cells represent the frequency or presence of terms.\n",
        "\n",
        "- **Document labels (classes)** are appended to this matrix.\n",
        "\n",
        "- The data is now ready for **document classification** using standard classification methods."
      ],
      "metadata": {
        "id": "ZT6-QopFVY3f"
      }
    }
  ]
}
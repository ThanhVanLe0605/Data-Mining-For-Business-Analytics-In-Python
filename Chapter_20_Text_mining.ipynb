{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFGOIhbKiUtajs5Uy0pLxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanhVanLe0605/Data-Mining-For-Business-Analytics-In-Python/blob/main/Chapter_20_Text_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we introduce text as a form of data.\n",
        "- First, we discuss a tabular representation of text data in which each column is a word, each row is a document, and each cell is a 0 or 1, indicating whether that column's word is present in that row's document.\n",
        "- Then we consider how to move from unstructured documents to this structured matrix.\n",
        "- Finnally, we illustrate how to integrate this process into the standard data mining procedures covered in earlier parts of the book."
      ],
      "metadata": {
        "id": "DnzhDww6s_uR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.1. [INTRODUCTION](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=_tud5FVMy6dV&line=1&uniqifier=1)\n",
        "\n",
        "20.2.[THE TABULAR REPRESENTATION OF TEXT: TERM-DOCUMENT MATRIX AND \"BAG-OF-WORDS\"](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=tlry-74ezIfL&line=1&uniqifier=1)\n",
        "\n",
        "20.3. [BAG-OF-WORDS VS. MEANING EXTRACTION AT DOCUMENT LEVEL](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=yqryZDJ1zdYq&line=1&uniqifier=1)\n",
        "\n",
        "20.4. [PREPROCESSING THE TEXT](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=Ulj1YLIWzvzW&line=1&uniqifier=1)\n",
        "\n",
        "20.5. [IMPLEMENTING DATA MINING METHODS](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=PcvJBFVQ0y9z&line=1&uniqifier=1)\n",
        "\n",
        "20.6. [EXAMPLE: ONLINE DISCUSSION ON AUTOS AND ELECTRONICS](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=eKYVcdku05FR&line=1&uniqifier=1)\n",
        "\n",
        "20.7. [SUMMARY](https://colab.research.google.com/drive/1rIv6HOfTRsYjsN7KnG_-WYKFBvIB4oaL#scrollTo=Xb7hf_1o1FRS&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "MI9tAHHry2IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python**\n",
        "\n",
        "In this chapter, we will **pandas** for data handling and **scikit-learn** for the feature creation and model building. The Natural Language Toolkit will be used for more advanced text processing (nltk: https://www.nltk.org)."
      ],
      "metadata": {
        "id": "FBwxRhsVuEOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required functionality for this chapter\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Install smba library as it's not found\n",
        "!pip install dmba\n",
        "\n",
        "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
        "\n",
        "# download data required for NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXd6y3s6wK0v",
        "outputId": "27407eaa-1db1-4a6b-836c-139fc73aa08c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dmba in /usr/local/lib/python3.12/dist-packages (0.2.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from dmba) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from dmba) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dmba) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dmba) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dmba) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from dmba) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->dmba) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.1. INTRODUCTION"
      ],
      "metadata": {
        "id": "_tud5FVMy6dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Traditional Data Types**\n",
        "Up to this point, data mining has primarily focused on structured data types:\n",
        "\n",
        "* a. **Numerical**\n",
        "* b. **Binary** (yes/no)\n",
        "\n",
        "* c. **Multicategory**\n",
        "\n",
        "**2. The Role of Text in Predictive Analytics**\n",
        "In many modern applications, data exists in **text form**. In these scenarios:\n",
        "\n",
        "* a. The **predictor variables (features)** are embedded directly within the text of documents.\n",
        "* b. **Automated algorithms** are required to process this unstructured data.\n",
        "\n",
        "**3. Real-world Classification Examples**\n",
        "* a. **Internet Service Providers (ISP):** Using algorithms to **classify** support tickets as *urgent* or *routine* for efficient routing.\n",
        "\n",
        "* b. **Legal Industry:** Automating the discovery process by classifying massive volumes of documents as *relevant* or *irrelevant*.\n",
        "\n",
        "**4. Drivers of Text Mining Growth**\n",
        "The field has expanded significantly due to the availability of **social media data** (Twitter feeds, blogs, online forums, news).\n",
        "\n",
        "* a. **Adoption Rate:** According to the *Rexer Analytics 2013 Data Mining Survey*, 25% to 40% of data miners utilize text data.\n",
        "\n",
        "* b. **Research Impact:** High public availability of this data provides a repository for researchers to hone and improve text mining methods.\n",
        "\n",
        "**5. Emerging Application Areas**\n",
        "\n",
        "* a. A key area of growth is applying text mining methods to **notes and transcripts** from contact centers and service centers."
      ],
      "metadata": {
        "id": "JeqkPg614eD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.2. THE TABULAR REPRESENTATION OF TEXT: TERM-DOCUMENT MATRIX AND \"BAG-OF-WORDS\""
      ],
      "metadata": {
        "id": "tlry-74ezIfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. The Term-Document Matrix**\n",
        "To analyze text quantitatively, unstructured sentences must be converted into a structured matrix format:\n",
        "* **Documents:** The distinct units of text (e.g., sentences $S1, S2, S3$).\n",
        "* **Terms:** The individual words extracted from these documents.\n",
        "* **Structure:** A matrix where rows represent terms and columns represent documents (or vice versa). The cell values represent the **frequency** of the term in that document.\n",
        "\n",
        "**2. Implementation in Python**\n",
        "* **Library:** `scikit-learn` (a standard machine learning library).\n",
        "* **Tool:** `CountVectorizer`.\n",
        "    * *Process:* Collect documents into a list $\\rightarrow$ Apply `CountVectorizer` to generate the matrix.\n",
        "\n",
        "**3. The \"Bag-of-Words\" (BoW) Approach**\n",
        "This matrix representation relies on the **Bag-of-Words** assumption:\n",
        "* **Ignored:** Grammar, syntax, and word order do not matter.\n",
        "* **Preserved:** Only the presence and frequency of words matter.\n",
        "* **Result:** Text is transformed into **tabular data** (numerical features) suitable for standard algorithms.\n",
        "\n",
        "\n",
        "\n",
        "**4. Applications and Challenges**\n",
        "* **Utility:** Once in tabular form, the data can be used for **Clustering**, **Classification**, or **Prediction** (by appending an outcome variable).\n",
        "* **Reality Check:** Text mining is rarely simple.\n",
        "    * Raw data usually requires significant **preprocessing** (cleaning).\n",
        "    * **Human review** is often indispensable to validate the relevance of documents before training."
      ],
      "metadata": {
        "id": "pw9Jr9Xj6yOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.1. TERM-DOCUMENT MATRIX REPRESENTATION OF WORDS IN SENTENCES S1-S3**"
      ],
      "metadata": {
        "id": "B-hKKEok7u3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    'this is the first sentence.',\n",
        "    'this is a second sentence.',\n",
        "    'the third sentence is here.'\n",
        "]\n",
        "# learn features based on text\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiIyxt7i66fQ",
        "outputId": "b9c30634-d13c-4de4-ef54-299089854ddc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          S1  S2  S3\n",
            "first      1   0   0\n",
            "here       0   0   1\n",
            "is         1   1   1\n",
            "second     0   1   0\n",
            "sentence   1   1   1\n",
            "the        1   0   1\n",
            "third      0   0   1\n",
            "this       1   1   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.3. BAG-OF-WORDS VS. MEANING EXTRACTION AT DOCUMENT LEVEL"
      ],
      "metadata": {
        "id": "yqryZDJ1zdYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Two Distinct Goals in Text Mining**\n",
        "We can categorize text mining tasks into two levels of complexity:\n",
        "* **Goal A: Classification & Clustering (The focus of this book)**\n",
        "    * *Task:* Labeling a document as belonging to a specific class or grouping similar documents together.\n",
        "    * *Method:* Uses standard statistical and machine learning predictive models (similar to those used for numerical data).\n",
        "    * *Requirement:* A sizable collection of documents (**Corpus**) and pre-labeled data for training.\n",
        "* **Goal B: Extracting Detailed Meaning**\n",
        "    * *Task:* Deriving deep understanding from a single document.\n",
        "    * *Method:* Requires complex algorithms to handle grammar, syntax, punctuation, and natural language logic.\n",
        "    * *Field:* This is the domain of **Natural Language Processing (NLP)**.\n",
        "\n",
        "**2. The Challenge of \"Meaning\"**\n",
        "Extracting meaning is far more formidable than probabilistic classification due to:\n",
        "* **Ambiguity:** Identical words can have vastly different meanings depending on context.\n",
        "* **Context Sensitivity:**\n",
        "    * *Example:* \"Hitchcock **shot** The Birds in Bodega Bay.\"\n",
        "    * *Interpretation 1:* Filming a movie (Correct in film context).\n",
        "    * *Interpretation 2:* Hunting birds (Possible interpretation if context is ignored).\n",
        "* **Resolution:** Humans use cultural and social context to resolve ambiguity; computers struggle with this in a simple Bag-of-Words model.\n",
        "\n",
        "\n",
        "\n",
        "**3. Scope of Analysis**\n",
        "* **In Scope:** We will focus on **probabilistic assignment**â€”using word frequencies to predict which class a document belongs to (e.g., \"Urgent\" vs. \"Routine\").\n",
        "* **Out of Scope:** We will not attempt to program the computer to \"understand\" the documents in the human sense (deep NLP)."
      ],
      "metadata": {
        "id": "gsStR4H5-PmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.4. PREPROCESSING THE TEXT"
      ],
      "metadata": {
        "id": "Ulj1YLIWzvzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Theory vs. Reality**\n",
        "* **Simple Theory:** Previous examples used clean text where words were separated by single spaces and sentences ended with periods. Simple rules could easily parse this.\n",
        "* **Complex Reality:** Real-world text data is \"messy.\" Preparing text for mining is significantly more involved than preparing numerical or categorical data.\n",
        "\n",
        "**2. Common Data Issues (The \"Dirty\" Examples)**\n",
        "Consider the modified sentences ($S1, S2, S3, S4$) presented in the text. They introduce typical noise found in raw data:\n",
        "\n",
        "  a. **Extra Spaces:** Irregular spacing between words.\n",
        "\n",
        "  b. **Non-alpha Characters:** Usage of punctuation and emoticons (e.g., `!!`, `:)`, `,`).\n",
        "\n",
        "  c. **Inconsistent Capitalization:** Random upper/lower case usage (e.g., \"Sentence\" capitalized in the middle).\n",
        "\n",
        "  d. **Misspellings:** Typographical errors (e.g., \"forth\" instead of \"fourth\").\n",
        "\n",
        "**3. The Concept of a \"Corpus\"**\n",
        "* **Definition:** A *corpus* refers to a large, fixed, and standard collection of documents.\n",
        "* **Purpose:**\n",
        "    a. Used to train text preprocessing algorithms.\n",
        "    \n",
        "    b. Serves as a benchmark to compare the results of different algorithms.\n",
        "* **Example:** The **Brown Corpus** (compiled at Brown University in the 1960s), which contains 500 English documents across various types."
      ],
      "metadata": {
        "id": "t61yApmzAOfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.2. TERM-DOCUMENT MATRIX REPRESENTATION OF WORDS IN SENTENCES S1-S4**"
      ],
      "metadata": {
        "id": "actXD2s5-ach"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "\n",
        "# Learn features based on text. Special characters are excluded in the analysis\n",
        "count_vext = CountVectorizer()\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu175tC9-deq",
        "outputId": "088222e1-fba2-41e9-c50e-48518d7f1972"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           S1  S2  S3  S4\n",
            "all         0   0   0   1\n",
            "first       1   0   0   0\n",
            "forth       0   0   0   1\n",
            "here        0   0   1   0\n",
            "is          1   1   1   0\n",
            "of          0   0   0   1\n",
            "second      0   1   0   0\n",
            "sentence    1   1   1   0\n",
            "sentences   0   0   0   1\n",
            "the         1   0   1   0\n",
            "third       0   0   1   0\n",
            "this        1   1   0   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "Mia-1iZPz9w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization (The First Step)**\n",
        "* **Definition:** The automated process of dividing raw text into separate units called \"tokens\" (or terms).\n",
        "* **Delimiters:** Software uses characters like spaces, commas, or colons to decide where one token ends and another begins.\n",
        "* **Result:** A raw list of terms that forms the basis of the Term-Document Matrix.\n",
        "\n",
        "**2. The Problem: \"Bulk and Noise\"**\n",
        "Creating a matrix from every single token leads to significant issues:\n",
        "* **Bulk (High Dimensionality):** The English language has over a million words. Including every date stamp, email address, or typo creates a massive, computationally heavy matrix.\n",
        "* **Noise (Irrelevant Data):** Many terms do not help in prediction and confuse the model.\n",
        "    * *Examples:* Boilerplate text (email signatures), random punctuation, or overly common words.\n",
        "\n",
        "\n",
        "\n",
        "**3. Text Reduction Strategies**\n",
        "To combat \"Bulk and Noise,\" preprocessing is essential:\n",
        "* **Stopwords Removal:** Eliminating common words (e.g., \"and\", \"the\", \"is\") that carry little semantic meaning for classification.\n",
        "* **Filtering Boilerplate:** Removing repetitive standard text (like legal disclaimers in emails) that appears in every document but adds no unique information.\n",
        "* **Goal:** To reduce the number of variables (columns) to only those that aid in analysis, making the model faster and more accurate."
      ],
      "metadata": {
        "id": "6Vyau79aBopW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.3 TOKENIZATION OF S1-S4 EXAMPLE**"
      ],
      "metadata": {
        "id": "Qh-WtkeFBzm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for term frequency of second example\n",
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "# learn features based on text. Include special characters that are part of a word in the analysis\n",
        "count_vect = CountVectorizer(token_pattern = ' [a-zA-Z!:]+')\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6rdtjilB5mK",
        "outputId": "5f98479e-2e07-47be-9770-5281b4783f75"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            S1  S2  S3  S4\n",
            ":            0   1   0   0\n",
            "a            0   1   0   0\n",
            "all          0   0   0   1\n",
            "first        1   0   0   0\n",
            "here         0   0   1   0\n",
            "is           1   1   1   0\n",
            "of           0   0   0   1\n",
            "second       0   1   0   0\n",
            "sentence     0   1   1   0\n",
            "sentence!!   1   0   0   0\n",
            "sentences    0   0   0   1\n",
            "the          1   0   0   0\n",
            "third        0   0   1   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Reduction**"
      ],
      "metadata": {
        "id": "ZLZaz5qm0BOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effective text reduction focuses on removing noise and reducing the vocabulary size to improve model performance. Below are the key techniques:\n",
        "\n",
        "### 1. Stopword Removal\n",
        "  a. **Tools:** Most software, such as the `CountVectorizer` class in `scikit-learn`, includes generic stopword lists for removing frequently occurring terms.\n",
        "  b. **Customization:** Users can review the extensive default list in `scikit-learn` or provide a custom list using the `stop_words` argument.\n",
        "\n",
        "### 2. Vocabulary Reduction Strategies\n",
        "Additional techniques to reduce text volume and focus on meaningful content include:\n",
        "\n",
        "* **Stemming**\n",
        "\n",
        "    a. A linguistic method that reduces different variants of words to a common core (root form).\n",
        "\n",
        "* **Frequency Filters**\n",
        "\n",
        "    a. Eliminate terms occurring in the great majority of documents (stop-words-like behavior).\n",
        "\n",
        "    b. Eliminate very rare terms to reduce noise.\n",
        "\n",
        "    c. Limit the vocabulary to the top *n* most frequent terms.\n",
        "\n",
        "* **Synonyms & Formatting**\n",
        "\n",
        "    a. Consolidate synonyms or synonymous phrases.\n",
        "\n",
        "    b. Ignore letter case (usually converting all text to lowercase).\n",
        "\n",
        "* **Normalization**\n",
        "\n",
        "    a. Replace specific terms within a category with the general category name.\n",
        "    \n",
        "    b. *Example:* Replacing distinct e-mail addresses with `emailtoken` or different numbers with `numbertoken`."
      ],
      "metadata": {
        "id": "x1VU0qFUDssM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Table 20.5 presents the text reduction step applied to the four sentences example, after tokenization. We can see the number of terms has been reduced to five"
      ],
      "metadata": {
        "id": "RybMPwAQD2HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Presence/Absence vs. Frequency**"
      ],
      "metadata": {
        "id": "aS3EBYxS0FHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Bag-of-Words\" model can be implemented in two ways:\n",
        "\n",
        "a. **Frequency-Based (Count)**\n",
        "   * Counts how many times a term appears.\n",
        "   * **Use Case:** When repetition implies intensity (e.g., repeated mentions of \"IP address\" in a support ticket indicate a specific technical issue).\n",
        "\n",
        "b. **Presence/Absence (Binary)**\n",
        "   * Records only if a term exists (1) or not (0), ignoring counts.\n",
        "   * **Use Case:** Classification tasks where the mere existence of a term is a key predictor (e.g., a specific vendor name in forensic accounting).\n",
        "   * **Implementation:** Set `binary=True` in `CountVectorizer`."
      ],
      "metadata": {
        "id": "u4l2Z9RhT060"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.4 STOPWORDS IN SCIKIT LEARN**"
      ],
      "metadata": {
        "id": "Ym-W5ERm4sWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
        "ncolumns = 6\n",
        "nrows = 30\n",
        "\n",
        "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
        "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
        "  print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMuxKev442Oq",
        "outputId": "4cab5cad-1c65-4a4e-c896-19bba7957594"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 180 of 318 stopwords\n",
            "a            about        above        across       after        afterwards   \n",
            "again        against      all          almost       alone        along        \n",
            "already      also         although     always       am           among        \n",
            "amongst      amoungst     amount       an           and          another      \n",
            "any          anyhow       anyone       anything     anyway       anywhere     \n",
            "are          around       as           at           back         be           \n",
            "became       because      become       becomes      becoming     been         \n",
            "before       beforehand   behind       being        below        beside       \n",
            "besides      between      beyond       bill         both         bottom       \n",
            "but          by           call         can          cannot       cant         \n",
            "co           con          could        couldnt      cry          de           \n",
            "describe     detail       do           done         down         due          \n",
            "during       each         eg           eight        either       eleven       \n",
            "else         elsewhere    empty        enough       etc          even         \n",
            "ever         every        everyone     everything   everywhere   except       \n",
            "few          fifteen      fifty        fill         find         fire         \n",
            "first        five         for          former       formerly     forty        \n",
            "found        four         from         front        full         further      \n",
            "get          give         go           had          has          hasnt        \n",
            "have         he           hence        her          here         hereafter    \n",
            "hereby       herein       hereupon     hers         herself      him          \n",
            "himself      his          how          however      hundred      i            \n",
            "ie           if           in           inc          indeed       interest     \n",
            "into         is           it           its          itself       keep         \n",
            "last         latter       latterly     least        less         ltd          \n",
            "made         many         may          me           meanwhile    might        \n",
            "mill         mine         more         moreover     most         mostly       \n",
            "move         much         must         my           myself       name         \n",
            "namely       neither      never        nevertheless next         nine         \n",
            "no           nobody       none         noone        nor          not          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Term Frequency-Inverse Document Frequency (TF_IDF)**"
      ],
      "metadata": {
        "id": "RziuGZXL0J9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n",
        "\n",
        "a. **The Concept**\n",
        "   * It highlights terms that are frequent in a specific document but rare across the entire corpus.\n",
        "\n",
        "b. **The Formula**\n",
        "   * **Term Frequency ($TF$):** Count of term $t$ in document $d$.\n",
        "   * **Inverse Document Frequency ($IDF$):**\n",
        "     $$IDF(t) = 1 + \\log\\left(\\frac{\\text{Total Documents}}{\\text{Documents containing } t}\\right)$$\n",
        "   * **Final Score:**\n",
        "     $$TF\\text{-}IDF = TF(t,d) \\times IDF(t)$$\n",
        "\n",
        "c. **Interpretation**\n",
        "   * **High score:** Rare term appearing frequently in the document.\n",
        "   * **Low score:** Term appearing in almost all documents (stopwords-like behavior) or absent terms."
      ],
      "metadata": {
        "id": "8aljJ_-AUDfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 20.5. Text redunction of S1-S4 (After tokenization)**"
      ],
      "metadata": {
        "id": "gOPMpBvi0Yte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for text reduction using stemming\n",
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "\n",
        "\n",
        "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing\n",
        "# (removes interpunctuation and stop words)\n",
        "class LemmaTokenizer(object):\n",
        "  def __init__(self):\n",
        "    self.stemmer = EnglishStemmer()\n",
        "    self.stopWords = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "  def __call__(self, doc):\n",
        "    return [self.stemmer.stem(t) for t in word_tokenize(doc)\n",
        "            if t.isalpha() and t not in self.stopWords]\n",
        "\n",
        "# Learn features based on text\n",
        "count_vect = CountVectorizer(tokenizer= LemmaTokenizer())\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiSn4Yd38DA_",
        "outputId": "bdd42e2c-3943-4a3a-cf80-58159eb216c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         S1  S2  S3  S4\n",
            "forth     0   0   0   1\n",
            "second    0   1   0   0\n",
            "sentenc   1   1   1   1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 20.6. TF-IDF MATRIX FOR S1-S4 EXAMPLE (AFTER TOKENIZATION AND TEXT REDUCTION)**"
      ],
      "metadata": {
        "id": "Tnyz41wv0hNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf transformation of term-document matrix\n",
        "text = [\n",
        "    'this is the first     sentence!!',\n",
        "    'this is a second sentence :',\n",
        "    'the third sentence, is here ',\n",
        "    'forth of all sentences'\n",
        "]\n",
        "\n",
        "# Apply countVectorizer and TfidfTransformer sequentially\n",
        "count_vect = CountVectorizer()\n",
        "tfidfTransformer = TfidfTransformer(smooth_idf = False, norm = None)\n",
        "counts = count_vect.fit_transform(text)\n",
        "tfidf = tfidfTransformer.fit_transform(counts)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SBr_nSd98zx",
        "outputId": "e7cf844b-06d8-45d9-a507-978e6718d333"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 S1        S2        S3        S4\n",
            "all        0.000000  0.000000  0.000000  2.386294\n",
            "first      2.386294  0.000000  0.000000  0.000000\n",
            "forth      0.000000  0.000000  0.000000  2.386294\n",
            "here       0.000000  0.000000  2.386294  0.000000\n",
            "is         1.287682  1.287682  1.287682  0.000000\n",
            "of         0.000000  0.000000  0.000000  2.386294\n",
            "second     0.000000  2.386294  0.000000  0.000000\n",
            "sentence   1.287682  1.287682  1.287682  0.000000\n",
            "sentences  0.000000  0.000000  0.000000  2.386294\n",
            "the        1.693147  0.000000  1.693147  0.000000\n",
            "third      0.000000  0.000000  2.386294  0.000000\n",
            "this       1.693147  1.693147  0.000000  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Terms to Concepts: Latent Semantic Indexing**"
      ],
      "metadata": {
        "id": "8L3a9E1E0qRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction: Latent Semantic Indexing (LSI)\n",
        "LSI (or LSA) reduces the complexity of text data by transforming \"Terms\" into \"Concepts\".\n",
        "\n",
        "a. **Mechanism**\n",
        "   * Similar to PCA (Principal Component Analysis), it groups correlated terms into linear combinations.\n",
        "   * **Example:** Terms like *alternator, battery, headlights* $\\rightarrow$ mapped to concept **\"Alternator Failure\"**.\n",
        "\n",
        "b. **Trade-off**\n",
        "   * **Pros:** Handles synonyms and reduces noise; improves manageability for modeling.\n",
        "   * **Cons:** Creates a **\"Blackbox\"** model. The resulting concepts may not always have a clear human-readable meaning, but they effectively cluster related documents."
      ],
      "metadata": {
        "id": "SLX7x1dLUPjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.5. IMPLEMENTING DATA MINING METHODS"
      ],
      "metadata": {
        "id": "PcvJBFVQ0y9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once text is converted into a numeric matrix, standard Data Mining methods are applied:\n",
        "\n",
        "a. **Clustering:** Grouping similar documents (e.g., clustering medical reports by symptoms).\n",
        "\n",
        "b. **Prediction:** Predicting continuous values (e.g., time to resolve a ticket).\n",
        "\n",
        "c. **Classification (Labeling):** Assigning categories to documents (the most common application)."
      ],
      "metadata": {
        "id": "NXOpQ9eFURB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.6. EXAMPLE: ONLINE DISCUSSION ON AUTOS AND ELECTRONICS"
      ],
      "metadata": {
        "id": "eKYVcdku05FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section outlines the end-to-end process of a text mining project, moving from raw text data to a predictive classification model.\n",
        "\n",
        "* **1. Problem Definition & Data Ingestion**\n",
        "    * a. **Goal**: Classify internet discussion posts into one of two categories: **Autos** (1) or **Electronics** (0).\n",
        "    * b. **Data Source**: A zipped collection of raw posts (emails/messages).\n",
        "    * c. **Ingestion**: Use Python's `ZipFile` module to read documents; create a label array based on the directory structure.\n",
        "\n",
        "* **2. Text Preprocessing Pipeline**\n",
        "    * a. **Tokenization**: Breaking down the text streams into individual words.\n",
        "    * b. **Stemming**: Reducing words to their root form (e.g., \"running\" $\\rightarrow$ \"run\").\n",
        "    * c. **Stopwords Removal**: Filtering out common, low-information words (e.g., \"the\", \"and\").\n",
        "\n",
        "* **3. Feature Engineering (Vectorization)**\n",
        "    * a. **TF-IDF Transformation**: Convert the \"clean\" corpus into a **Term-Document Matrix**.\n",
        "    * b. **Weighting**: Use **TF-IDF** (Term Frequency-Inverse Document Frequency) to evaluate the importance of a word within a document relative to the entire corpus.\n",
        "    * c. **Tool**: Implementation using `TfidfTransformer` in Python.\n",
        "\n",
        "* **4. Dimensionality Reduction (Latent Semantic Indexing - LSI)**\n",
        "    * a. **Challenge**: The resulting TF-IDF matrix is extremely sparse and high-dimensional (approx. 13,466 predictors), making it computationally expensive.\n",
        "    * b. **Solution**: Apply **LSI** to extract a reduced set of \"concepts\" (reducing features from ~13k to **20 concepts**).\n",
        "    * c. **Implementation**: Use `TruncatedSVD` followed by `Normalizer` in `scikit-learn`.\n",
        "\n",
        "* **5. Model Fitting & Evaluation**\n",
        "    * a. **Data Partitioning**: Split the processed data into **60% Training** and **40% Validation**.\n",
        "    * b. **Algorithm**: Train a **Logistic Regression** model using the 20 concept variables as predictors.\n",
        "    * c. **Performance**:\n",
        "        * **Confusion Matrix**: Achieved high accuracy (**0.96**).\n",
        "        * **Lift Chart**: Validated high separability between classes (Max lift ~2 in the top deciles).\n",
        "\n",
        "* **6. Extended Application (Numeric Prediction)**\n",
        "    * a. Text mining is not limited to classification (\"labeling\").\n",
        "    * b. It can be adapted for **Regression** tasks to predict numerical values (e.g., predicting *repair costs* or *length of service* based on maintenance logs).\n",
        "    * c. The preprocessing steps remain the same; only the target variable changes from a categorical class to a continuous numeric value."
      ],
      "metadata": {
        "id": "4zsd39GzNL4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing and Labeling the Records**"
      ],
      "metadata": {
        "id": "muAK-RU8_06x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.7 IMPORTING AND LEBELING THE RECORDS, PREPROCESSING TEXT, AND PRODUCING CONCEPT MATRIX**"
      ],
      "metadata": {
        "id": "98CVxLxp_8j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for importing and labeling records, preprocessing text, and producing concept matrix\n",
        "\n",
        "# Step 1: import and label records\n",
        "import tarfile # Import tarfile module for .tar.gz files\n",
        "\n",
        "corpus = []\n",
        "label = []\n",
        "# It's good practice to wrap file operations in a try-except block for robustness\n",
        "try:\n",
        "  # Use tarfile.open for .tar.gz files, 'r:gz' mode for gzip compressed tar archive\n",
        "  with tarfile.open('AutoAndElectronics.tar.gz', 'r:gz') as rawData:\n",
        "    for info in rawData:\n",
        "      # Ensure we are processing actual files and not directories within the tar archive\n",
        "      if info.isfile():\n",
        "        # Determine label based on filename (e.g., 'rec.autos' vs 'rec.electronics')\n",
        "        # info.name contains the full path within the archive\n",
        "        label.append(1 if 'rec.autos' in info.name else 0)\n",
        "        # Read content from the file inside the tar archive and decode it using 'latin1'\n",
        "        # 'latin1' is used here to match the encoding specified for CountVectorizer.\n",
        "        content = rawData.extractfile(info).read().decode('latin1')\n",
        "        corpus.append(content)\n",
        "except Exception as e:\n",
        "  print(f\"Error processing tar.gz file: {e}\")\n",
        "  # Depending on how you want to handle this, you might re-raise or exit.\n",
        "  # For now, just print the error and continue, but corpus will be empty.\n",
        "  pass\n",
        "\n",
        "# Step 2: preprocessing (tokenization, stemming, and stopwords)\n",
        "class LemmaTokenizer(object):\n",
        "  def __init__(self): # Corrected __ini__ to __init__\n",
        "    self.stemmer = EnglishStemmer()\n",
        "    # Using nltk stopwords as ENGLISH_STOP_WORDS from sklearn is deprecated\n",
        "    self.stopWords = set(stopwords.words('english')) # Use nltk stopwords\n",
        "\n",
        "  def __call__(self, doc):\n",
        "    return [self.stemmer.stem(t) for t in word_tokenize(doc)\n",
        "            if t.isalpha() and t not in self.stopWords]\n",
        "\n",
        "# Only proceed with preprocessing if corpus is not empty\n",
        "if corpus:\n",
        "  preprocessor = CountVectorizer(tokenizer = LemmaTokenizer(), encoding ='latin1')\n",
        "  preprocessedText = preprocessor.fit_transform(corpus)\n",
        "\n",
        "\n",
        "  # Step 3: TF_IDF and latent semantic analysis\n",
        "  tfidfTransformer = TfidfTransformer()\n",
        "  tfids = tfidfTransformer.fit_transform(preprocessedText)\n",
        "\n",
        "  # Extract 20 concepts using LSA\n",
        "  svd = TruncatedSVD(20)\n",
        "  normalizer = Normalizer(copy=False)\n",
        "  lsa = make_pipeline(svd, normalizer) # Corrected assignment operator from '-' to '='\n",
        "\n",
        "  lsa_tfidf = lsa.fit_transform(tfids)\n",
        "\n",
        "else:\n",
        "  print(\"Corpus is empty, skipping text preprocessing.\")\n",
        "  preprocessedText = None # Or handle appropriately if corpus is empty\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aai6V9FLAGRB",
        "outputId": "b8565685-a583-472f-b029-1a0180ba73e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 20.8 FITTING A PREDICTIVE MODEL TO THE AUTOS AND ELECTRONICS DISCUSSION DATA**"
      ],
      "metadata": {
        "id": "VX4146oVGKmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for fitting and evaluating a logistic regression predictive model\n",
        "\n",
        "# split dataset into 60% training and 40% test set\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size =0.4, random_state = 42)\n",
        "\n",
        "# run logistic regression model on training\n",
        "logit_reg = LogisticRegression(solver = 'lbfgs')\n",
        "logit_reg.fit(Xtrain, ytrain)\n",
        "\n",
        "# print confusion matrix and accuracy\n",
        "classificationSummary(ytest, logit_reg.predict(Xtest))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "544cJlE9GIIu",
        "outputId": "7c916feb-c70a-43a4-f6d8-57dece81c7e7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Accuracy 0.9669)\n",
            "\n",
            "       Prediction\n",
            "Actual    0    1\n",
            "     0 7508   83\n",
            "     1  182  226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf8e7d7"
      },
      "source": [
        "## Figure 20.1: Term-Document Matrix for Sentences S1-S3\n",
        "\n",
        "This code block generates the term-document matrix as depicted in Figure 20.1, illustrating the representation of words in sentences S1-S3. Each row represents a term, each column represents a sentence, and the cell values indicate the frequency of a term in a given sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20.7. SUMMARY"
      ],
      "metadata": {
        "id": "Xb7hf_1o1FRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distinction between NLP and Text Mining**\n",
        "\n",
        "- **Natural Language Processing (NLP):** Focuses on extracting meaning from a single document.\n",
        "- **Text Mining:** Focuses on classifying or labeling numerous documents in a probabilistic fashion.\n",
        "- *Note:* This chapter concentrates on Text Mining.\n",
        "\n",
        "**Preprocessing Challenges**\n",
        "\n",
        "- Preprocessing text is more varied and involved than preparing numerical data.\n",
        "- The ultimate goal is to produce a matrix where rows represent **terms** and columns represent **documents**.\n",
        "\n",
        "**Dimensionality Reduction**\n",
        "\n",
        "- **Vocabulary Reduction:** Necessary because the sheer number of terms in natural language is excessive for effective model-building.\n",
        "\n",
        "- **Concept Extraction:** A final major reduction involves using a limited set of *concepts* instead of raw terms.\n",
        "\n",
        "- **Analogy:** This captures variation in documents similarly to how *Principal Components* capture variation in numerical data.\n",
        "\n",
        "**Final Output & Application**\n",
        "\n",
        "- The process results in a **quantitative matrix** where cells represent the frequency or presence of terms.\n",
        "\n",
        "- **Document labels (classes)** are appended to this matrix.\n",
        "\n",
        "- The data is now ready for **document classification** using standard classification methods."
      ],
      "metadata": {
        "id": "ZT6-QopFVY3f"
      }
    }
  ]
}
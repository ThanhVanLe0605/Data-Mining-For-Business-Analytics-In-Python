{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkvTC+pGnz+ayQdktmTl6U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanhVanLe0605/Data-Mining-For-Business-Analytics-In-Python/blob/main/Chapter_10_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOGISTIC REGRESSION\n"
      ],
      "metadata": {
        "id": "wiEIEYJUthJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LOGISTIC REGRESSION OVERVIEW](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=1WUkAZJzubtP&line=1&uniqifier=1)\n",
        "\n",
        "10.1. [INTRODUCTION](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=eMDc_ZO-xgni&line=1&uniqifier=1)\n",
        "\n",
        "10.2. [Logistic Regression: The Math Behind the model](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=T6PKwUll25Ec&line=1&uniqifier=1)\n",
        "\n",
        "10.3. [Example: Acceptance of Personal Loan](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=mT1o6tNJ7PEc&line=68&uniqifier=1)\n",
        "\n",
        "10.4. [Evaluating Classification Performance](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=fQLDXLQIFid_&line=46&uniqifier=1)\n",
        "\n",
        "10.5. [Multi-class Logistic Regression](https://colab.research.google.com/drive/1rhDM8FPZFLhMkj6Yo0xeRjnCvGXI_pf3#scrollTo=4jTDgoJHHQf8&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "_DkxCCM8ula_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOGISTIC REGRESSION OVERVIEW"
      ],
      "metadata": {
        "id": "1WUkAZJzubtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section introduces **Logistic Regression**, a highly popular and powerful method used for classification tasks.\n",
        "\n",
        "* **Methodology & Setup**\n",
        "    * It models the relationship between **predictors** and a specific **outcome**, similar to linear regression.\n",
        "    * Users must explicitly specify predictors and their forms (e.g., interaction terms).\n",
        "\n",
        "* **Key Advantages**\n",
        "    * a. Effective even on **small datasets**.\n",
        "    * b. **Computationally efficient**: Once the model is estimated, classifying large samples of new records is fast and cheap.\n",
        "\n",
        "* **Core Concepts & Estimation**\n",
        "    * Focuses on model formulation and estimation from data.\n",
        "    * Explains the fundamental relations between **\"logit\"**, **\"odds\"**, and **\"probability\"** of an event.\n",
        "\n",
        "* **Advanced Topics**\n",
        "    * a. **Variable importance** and **coefficient interpretation**.\n",
        "    * b. **Variable selection** techniques for **dimension reduction**.\n",
        "    * c. Extensions to **multi-class classification** problems."
      ],
      "metadata": {
        "id": "8r2eOUHku0c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python\n",
        "\n",
        "In this chapter, we will use `pandas` for data handling, `scikit-learn` and `statsmodels` for the models, and `matplotlib` for visualization. We will also make use of the utility functions from the Python Utilities Functions Appendix. Use the following import statements for the Python code in this chapter.\n",
        "\n",
        "```python\n",
        "# import required functionality for this chapter"
      ],
      "metadata": {
        "id": "AipQREgEv4eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFVIZnyshCP",
        "outputId": "c1b3ddb2-bc61-4df3-88d9-4753f669392a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mord in /usr/local/lib/python3.12/dist-packages (0.7)\n",
            "Requirement already satisfied: dmba in /usr/local/lib/python3.12/dist-packages (0.2.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from dmba) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from dmba) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dmba) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dmba) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dmba) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from dmba) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->dmba) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# import required functionality for this chapter\n",
        "!pip install mord dmba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm\n",
        "from mord import LogisticIT\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dmba import classificationSummary, gainsChart, liftChart\n",
        "from dmba.metric import AIC_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1. INTRODUCTION\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eMDc_ZO-xgni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This section details the scope, application, and mechanics of the Logistic Regression model.\n",
        "\n",
        "* **Core Concept & Application**\n",
        "    * a. **Purpose**: Extends linear regression to handle **categorical outcomes** (classes) rather than continuous values.\n",
        "    * b. **Primary Uses**:\n",
        "        * **Classification**: Predicting the class of a new record based on predictors.\n",
        "        * **Profiling**: Identifying factors that distinguish between classes in known data.\n",
        "    * c. **Target Variable**: Focuses primarily on **binary outcomes** (e.g., Success/Failure, 0/1). Continuous variables are sometimes converted (binned) into binary classes for simplification.\n",
        "\n",
        "* **Mechanism: Propensities and Cutoffs**\n",
        "    Logistic regression operates in two distinct steps to classify records:\n",
        "    * a. **Step 1: Estimation**: The model calculates the **propensity** (or probability) that a record belongs to the class of interest, denoted as p= (Y=1).\n",
        "    * b. **Step 2: Classification via Cutoff**:\n",
        "        * A **cutoff value** is applied to the estimated probabilities to assign classes.\n",
        "        * **Standard Rule**: Typically, if $P(Y=1) \\ge 0.5$, the record is classified as Class 1.\n",
        "        * **Adjustments**: For rare but critical events (e.g., fraud), the cutoff may be lowered to capture more Class 1 cases."
      ],
      "metadata": {
        "id": "dHq3FVWs3jIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2. Logistic Regression: The Math Behind the model:\n"
      ],
      "metadata": {
        "id": "T6PKwUll25Ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This section explains the mathematical formulation linking predictors to the probability of an outcome.\n",
        "\n",
        "* **The Limitation of Linear Regression**\n",
        "    * Standard linear regression cannot be used directly for classification because it may predict values outside the required probability range of [0, 1].\n",
        "    * **Solution**: Use a nonlinear function (Logistic Response Function) to ensure predictions stay within [0, 1].\n",
        "\n",
        "* **Key Concepts & Relationships**\n",
        "    * a. **Probability ($p$)**:\n",
        "        * The probability of belonging to class 1: p = P(Y=1).\n",
        "        * **Range**: [0, 1].\n",
        "        * **Formula**: p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_q x_q)}}\n",
        "    * b. **Odds**:\n",
        "        * The ratio of the probability of the event happening to it *not* happening.\n",
        "        * **Formula**: $\\text{Odds}(Y=1) = \\frac{p}{1-p}$\n",
        "        * **Range**: $[0, \\infty)$ (from 0 to infinity).\n",
        "        * **Relationship with predictors**: Multiplicative (exponential).\n",
        "    * c. **Logit (Log-Odds)**:\n",
        "        * The natural logarithm of the odds.\n",
        "        * **Formula**: $\\text{logit} = \\log(\\text{Odds}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_q x_q$\n",
        "        * **Range**: $(-\\infty, +\\infty)$.\n",
        "        * **Relationship with predictors**: **Linear**. This allows us to use linear regression techniques to estimate the coefficients.\n",
        "\n",
        "* **Summary of Transformation Steps**\n",
        "    1.  Predictors ($X$) $\\rightarrow$ Linear Equation $\\rightarrow$ **Logit**\n",
        "    2.  **Logit** $\\rightarrow$ Exponentiation ($e^{logit}$) $\\rightarrow$ **Odds**\n",
        "    3.  **Odds** $\\rightarrow$ Mapping ($\\frac{Odds}{1+Odds}$) $\\rightarrow$ **Probability ($p$)**"
      ],
      "metadata": {
        "id": "Nz6ssuqo6UFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 Example: Acceptance of Personal Loan\n",
        "Logistic Regression: Personal Loan Acceptance Case Study\n",
        "\n",
        "###**1. Context & Problem Definition**\n",
        "\n",
        "a. **Data Context:** The Universal Bank dataset contains 5000 customer records.\n",
        "\n",
        "b. **Target Variable ($Y$):** `Personal Loan` (Binary: Did the customer accept the loan offer in the last campaign?).(\n",
        "\n",
        "c. **Statistics:** Only 480 customers (9.6%) accepted the loan (imbalanced classes).\n",
        "\n",
        "d. **Objective:** Build a **classification model** to identify customers most likely to accept a loan offer in future campaigns.\n",
        "\n",
        "---\n",
        "\n",
        "###**2. Model with a Single Predictor**\n",
        "\n",
        "a. **Concept:** Similar to simple linear regression, but the outcome variable $Y$ is categorical.\n",
        "\n",
        "b. **Predictor ($X$):** Using `Income` to classify customers.\n",
        "\n",
        "c. **Probability Formula ($P$):** The probability of accepting a loan given income $x$ is calculated using the Logistic function (Sigmoid):\n",
        "   $$P(Personal Loan = Yes | Income = x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}$$\n",
        "\n",
        "d. **Odds Formula:**\n",
        "   $$\\text{Odds}(Personal Loan = Yes | Income = x) = e^{\\beta_0 + \\beta_1 x}$$\n",
        "\n",
        "\n",
        "e. **Classification Mechanism:**\n",
        "   - The model outputs a probability $p$ between 0 and 1.\n",
        "   - A **cutoff value** is applied to classify the result as 1 (Accept) or 0 (Reject). If $p > \\text{cutoff}$, predict 1.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "###**3. Estimating the Model (MLE vs. Least Squares)**\n",
        "\n",
        "a. **Methodology:** Unlike Linear Regression which uses *Least Squares*, Logistic Regression uses **Maximum Likelihood Estimation (MLE)**.\n",
        "\n",
        "b. **Reasoning:** The relationship between the target $Y$ and parameters $\\beta$ is non-linear.\n",
        "\n",
        "c. **MLE Principle:** Finds the parameters that maximize the chance (likelihood) of obtaining the observed data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**4. Data Preprocessing**\n",
        "\n",
        "a. **Categorical Variables:**\n",
        "   - Variables like `Education` (levels 1, 2, 3) must be converted into **dummy variables** (one-hot encoding).\n",
        "   - **Multicollinearity Note:** To avoid perfect multicollinearity, keep only $k-1$ dummy variables for $k$ categories.\n",
        "b. **Data Splitting:**\n",
        "   - **Training set:** 60% (for model fitting).\n",
        "   - **Validation set:** 40% (for performance evaluation).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "###**5. Estimated Model & Logit Equation**\n",
        "\n",
        "a. **The Logit:** The natural logarithm of Odds ($ln(Odds)$). It has a **linear relationship** with the predictors, allowing us to see the additive effect of variables.\n",
        "\n",
        "b. **Estimated Equation (12 Predictors):**\n",
        "   $$\n",
        "   \\begin{aligned}\n",
        "   \\text{Logit}(Personal Loan = Yes) = & -12.619 - 0.0325(\\text{Age}) + 0.0342(\\text{Experience}) \\\\\n",
        "   & + 0.0588(\\text{Income}) + 0.6141(\\text{Family}) + 0.2405(\\text{CCAvg}) \\\\\n",
        "   & + 0.0010(\\text{Mortgage}) - 1.0262(\\text{Securities\\_Account}) \\\\\n",
        "   & + 3.6479(\\text{CD\\_Account}) - 0.6779(\\text{Online}) - 0.9560(\\text{Credit Card}) \\\\\n",
        "   & + 4.1922(\\text{Education\\_Graduate}) \\\\\n",
        "   & + 4.3417(\\text{Education\\_Advanced/Professional})\n",
        "   \\end{aligned}\n",
        "   $$\n",
        "c. **Interpretation:**\n",
        "   - **Positive coefficients** (e.g., `Income`, `CD_Account`) increase the probability of loan acceptance.\n",
        "   - **Negative coefficients** (e.g., `Credit Card`, `Online`) decrease the probability.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "###**6. Interpreting Results via Odds Ratios**\n",
        "\n",
        "a. **Odds Ratio Formula:**\n",
        "   $$\\text{Odds Ratio} = e^{\\beta_1}$$\n",
        "b. **Meaning:** $e^{\\beta_1}$ is the multiplicative factor impact on the Odds when $X_1$ increases by 1 unit (holding other variables constant).\n",
        "   - If $\\beta_1 > 0$: $e^{\\beta_1} > 1$ (Odds increase).\n",
        "   - If $\\beta_1 < 0$: $e^{\\beta_1} < 1$ (Odds decrease).\n",
        "c. **Examples:**\n",
        "   - **Income ($\\beta \\approx 0.036$):** A 1-unit increase in Income increases the odds of acceptance by a factor of $e^{0.036}$.\n",
        "   - **CD_Account ($\\beta \\approx 3.65$):** Customers with a CD Account have odds of acceptance approx. **38.4 times** ($e^{3.65}$) higher than those without."
      ],
      "metadata": {
        "id": "mT1o6tNJ7PEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 Evaluating Classification Performance\n",
        "\n",
        "### 1. Evaluating Classification Performance\n",
        "To assess how well a Logistic Regression model performs, we use several metrics and visualizations beyond simple accuracy.\n",
        "\n",
        "* **Key Metrics:**\n",
        "    * **Confusion Matrix:** A table showing True Positives, True Negatives, False Positives, and False Negatives.\n",
        "    * **Accuracy:** The overall percentage of correct predictions.\n",
        "    * **Ranking Goal:** In many business cases (e.g., credit scoring), ranking customers by their probability of belonging to a class is more important than just classifying them.\n",
        "\n",
        "* **Visualizations:**\n",
        "    * **Gains Chart & Lift Chart:** These evaluate the model's ability to identify targets compared to random selection.\n",
        "        * *Interpretation:* A \"Lift\" implies how much better the model is at identifying the target class compared to a naive baseline. For example, the top 10% of customers identified by the model might contain 7.8 times more actual responders than a random 10% sample.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Interpreting Model Output\n",
        "Understanding the relationship between predictors and the outcome is crucial.\n",
        "\n",
        "* **Coefficients ($\\beta$):** Represent the change in the **Logit** (log-odds) for a one-unit increase in the predictor.\n",
        "    * Positive $\\beta$: Increases the probability of the event.\n",
        "    * Negative $\\beta$: Decreases the probability of the event.\n",
        "* **Odds Ratios (O.R. = $e^{\\beta}$):** A more intuitive measure.\n",
        "    * *Interpretation:* If O.R. = 1.05, it means a one-unit increase in the predictor increases the odds of the event by 5%.\n",
        "    * If O.R. > 1: Positive relationship.\n",
        "    * If O.R. < 1: Negative relationship.\n",
        "* **P-values:** Determine statistical significance. A low p-value (typically < 0.05) indicates the predictor is significantly related to the outcome (e.g., *Income* and *Education* in the bank loan example).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Variable Selection & Model Validation\n",
        "Finding the right balance between model simplicity (parsimony) and accuracy.\n",
        "\n",
        "* **Selection Methods:**\n",
        "    * **Automated Heuristics:** Stepwise, Forward Selection, and Backward Elimination (often minimizing **AIC**).\n",
        "    * **Regularization:** Using **L1 (Lasso)** or **L2 (Ridge)** penalties to prevent overfitting. In Python, this is controlled by the `C` parameter (inverse of regularization strength).\n",
        "    * **Interaction Terms:** Adding terms like $Income \\times Family$   if variables have combined effects.\n",
        "\n",
        "* **Profiling via Deciles:**\n",
        "    * Analyzing the characteristics of the \"Top Decile\" (top 10% highest probability) vs. the overall average helps build a profile of the target audience (e.g., \"Targets have higher income and education\").\n",
        "\n",
        "* **CRITICAL NOTE: The Danger of \"Overly Optimistic\" Performance**\n",
        "    * **The Issue:** Relying solely on **Validation Data** for performance evaluation can be misleading. Since validation data is used to *select* the best model (tuning), the model implicitly \"learns\" the specific noise of the validation set.\n",
        "    * **The Solution:** Always reserve a separate **Test Set** (Unseen Data) that is never used during the training or model selection process. This provides an unbiased estimate of how the model will perform in the real world.\n",
        "\n"
      ],
      "metadata": {
        "id": "fQLDXLQIFid_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.5  Multi-class Logistic Regression\n",
        "\n",
        "\n",
        "When the target variable has more than two classes ($m > 2$), the binary logistic model is extended. Since the sum of probabilities must equal 1, we estimate $m-1$ probabilities.\n",
        "\n",
        "### 1. Ordinal Classes (Ordered Categories)\n",
        "Used when classes have a meaningful order (e.g., *Buy, Hold, Sell* or *Low, Medium, High*). The method is often called **Cumulative Logit** or **Proportional Odds**.\n",
        "\n",
        "* **Key Concept:** Model the cumulative probability $P(Y \\le j)$.\n",
        "* **Assumption:** The predictors have the **same slope ($\\beta$)** across all class levels, but different intercepts ($\\alpha$).\n",
        "\n",
        "**Formulas (Example with $m=3$ classes):**\n",
        "The logit functions for the cumulative probabilities are:\n",
        "$$\n",
        "\\text{logit}(Y \\le 1) = \\ln \\left( \\frac{P(Y \\le 1)}{1 - P(Y \\le 1)} \\right) = \\alpha_0 + \\beta_1 x\n",
        "$$\n",
        "$$\n",
        "\\text{logit}(Y \\le 2) = \\ln \\left( \\frac{P(Y \\le 2)}{1 - P(Y \\le 2)} \\right) = \\beta_0 + \\beta_1 x\n",
        "$$\n",
        "\n",
        "**Recovering Probabilities:**\n",
        "$$\n",
        "P(Y=1) = \\frac{1}{1 + e^{-(\\alpha_0 + \\beta_1 x)}}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "P(Y=2) = P(Y \\le 2) - P(Y \\le 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} - P(Y=1)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "P(Y=3) = 1 - P(Y \\le 2)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Nominal Classes (Unordered Categories)\n",
        "Used when classes have no intrinsic order (e.g., *Brand A, Brand B, Brand C*). We use **Multinomial Logistic Regression**.\n",
        "\n",
        "* **Key Concept:** Select one class as the **Reference Class** (e.g., Class C). Model the log-odds of membership in other classes relative to the reference.\n",
        "* **Assumption:** Each class comparison has its **own unique set of coefficients** (different slopes and intercepts).\n",
        "\n",
        "**Formulas (Example with Reference = C):**\n",
        "The \"pseudo-logit\" equations are:\n",
        "$$\n",
        "\\text{logit}(A) = \\ln \\left( \\frac{P(Y=A)}{P(Y=C)} \\right) = \\alpha_0 + \\alpha_1 x\n",
        "$$\n",
        "$$\n",
        "\\text{logit}(B) = \\ln \\left( \\frac{P(Y=B)}{P(Y=C)} \\right) = \\beta_0 + \\beta_1 x\n",
        "$$\n",
        "\n",
        "**Recovering Probabilities (Softmax):**\n",
        "$$\n",
        "P(Y=A) = \\frac{e^{\\alpha_0 + \\alpha_1 x}}{1 + e^{\\alpha_0 + \\alpha_1 x} + e^{\\beta_0 + \\beta_1 x}}\n",
        "$$\n",
        "$$\n",
        "P(Y=B) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\alpha_0 + \\alpha_1 x} + e^{\\beta_0 + \\beta_1 x}}\n",
        "$$\n",
        "$$\n",
        "P(Y=C) = 1 - P(Y=A) - P(Y=B)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "4jTDgoJHHQf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Summary Table: Ordinal vs. Nominal\n",
        "\n",
        "| Feature | Ordinal Logistic Regression | Nominal Logistic Regression |\n",
        "| :--- | :--- | :--- |\n",
        "| **Use Case** | Ranked data (Severity, Ratings) | Unordered data (Brands, Types) |\n",
        "| **Slopes ($\\beta$)** | **Shared** (Parallel lines assumption) | **Separate** for each class |\n",
        "| **Intercepts ($\\alpha$)** | Separate | Separate |\n",
        "| **Complexity** | More parsimonious (fewer parameters) | More complex (more parameters) |"
      ],
      "metadata": {
        "id": "GlodWojZJP72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.6. Example of complete analysis: predicting delayed flights\n",
        "\n"
      ],
      "metadata": {
        "id": "VQ5StHaGINY-"
      }
    }
  ]
}
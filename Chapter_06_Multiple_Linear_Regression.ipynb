{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDKyl+Jh7NyT4hfqnwI81x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanhVanLe0605/Data-Mining-For-Business-Analytics-In-Python/blob/main/Chapter_06_Multiple_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$CHAPTER$ $ $ $6:$ $MULTIPLE$ $ $ $LINEAR$ $ $ $REGRESSION$"
      ],
      "metadata": {
        "id": "1qBpB9pKM3F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chapter introduces linear regression models with a specific focus on predictive analytics. Key topicscovered include:\n",
        "\n",
        "* **Linear Regression Models**\n",
        "\n",
        "Introduction to regression models designed for **prediction**\n",
        "\n",
        "Distinguish between model fitting for **inference**(classical statistics) and fitting for **prediction**\n",
        "\n",
        "* **Model Evaluation Methodology**\n",
        "\n",
        "The necessity of evaluating model performance on a **validation set**\n",
        "\n",
        "The use of specific **predictive metrics**(e.g, RMSE, MAE) rather than goodness-of-fit statistics  alone ( like $R^{2}$ )\n",
        "\n",
        "* **Variable Selection**\n",
        "\n",
        "Addressing the challenges associated with using a large number of **predictors** (features).\n",
        "\n",
        "Implementation of **Variable Selection Algorithms** to identify the most relevant features for the model."
      ],
      "metadata": {
        "id": "yyQYevXPNh09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TABLE OF CONTENTS\n",
        "\n",
        "6.1. [INTRODUCTION](https://colab.research.google.com/drive/1x2NpXouzqo869hcjUJJ9aW59Fqbhr78k#scrollTo=IB8Z828xSRLb&line=1&uniqifier=1)\n",
        "\n",
        "6.2. [EXPLANATORY VS. PREDICTIVE MODELING](https://colab.research.google.com/drive/1x2NpXouzqo869hcjUJJ9aW59Fqbhr78k#scrollTo=XBNBFMp6SWuR&line=1&uniqifier=1)\n",
        "\n",
        "6.3. [ESTIMATING THE REGRESSION EQUATION AND PREDICTION](https://colab.research.google.com/drive/1x2NpXouzqo869hcjUJJ9aW59Fqbhr78k#scrollTo=p143lu77Sco2&line=1&uniqifier=1)\n",
        "\n",
        "6.4. [VARIABLE SELECTION IN LINEAR REGRESSION](https://colab.research.google.com/drive/1x2NpXouzqo869hcjUJJ9aW59Fqbhr78k#scrollTo=7VFkjt6tTfQ-&line=1&uniqifier=1)\n",
        "\n",
        "6.5. [APPENDIX: USING STATMODELS](https://colab.research.google.com/drive/1x2NpXouzqo869hcjUJJ9aW59Fqbhr78k#scrollTo=Hugi2j4RU6Na&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "3xtzlgCAR0fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python**\n",
        "\n",
        "In this chapter, we will use **pandas** for data handling, and **scikit-learn** for building the models, and variable (feature) selection. We will also make use of the utility functions from the Python Utilities Functions Appendix. We could use **statmodels** for the linear regression model, however, **statmodels** provides more information than needed for predictive modeling. Use the following import statements for the Python code in this chapter.\n"
      ],
      "metadata": {
        "id": "1XMcZwZIKKtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dmba\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge\n",
        "import statsmodels.formula.api as sm\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from dmba import regressionSummary, exhaustive_search\n",
        "from dmba import backward_elimination, forward_selection, stepwise_selection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIEFG2VCK_Nt",
        "outputId": "629bcb02-d909-4212-9909-629cd5c818d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dmba\n",
            "  Downloading dmba-0.2.4-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from dmba) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from dmba) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dmba) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dmba) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dmba) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from dmba) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->dmba) (1.17.0)\n",
            "Downloading dmba-0.2.4-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dmba\n",
            "Successfully installed dmba-0.2.4\n",
            "Colab environment detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1. INTRODUCTION"
      ],
      "metadata": {
        "id": "IB8Z828xSRLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6.1.1. Model Definition & Terminology**\n",
        "* The **multiple linear regression model** is the most popular model for making predictions. It fits a relationship between a numerical outcome and a set of predictors.\n",
        "\n",
        "* **Outcome  variable ($Y$):** Also called the response, target, or dependent variable.\n",
        "\n",
        "* **Predictors ($X_1, \\dots, X_p$):** Also called independent variables, input variables, regressors, or covariates.\n",
        "\n",
        "#### **6.1.2. The Model Equation**\n",
        "The model assumes the relationship is approximated by the function:\n",
        "\n",
        "$$Y= \\beta_0 +\\beta_1x_1 +\\beta_2x_2 +\\dots +\\beta_px_p +\\epsilon$$\n",
        "\n",
        "* $\\beta_0, \\dots, \\beta_p$: **Coefficients** estimated from the data.\n",
        "\n",
        "* $\\epsilon$: **Noise** (unexplained part)\n",
        "#### **6.1.3. Feature Engineering (Input Forms)**\n",
        "Regression modeling involves not only estimating coefficients but also selecting **which predictors** to include and in **what form**.\n",
        "\n",
        "* **Forms:** Predictors can be included 'as is', in logarithmic form [$\\log(X)$], or in binned form (e.g., are groups)\n",
        "* **Selection Criteria:** Choices depend on **domain knowledge**, data availability, and needed predictive power.\n",
        "\n",
        "\n",
        "#### **6.1.4. Applications**\n",
        "\n",
        "Common predictive modeling situations include:\n",
        "* Predicting credit card activity based on demographics and historical patterns.\n",
        "* Predicting vacation travel expenditures based on frequent flyer data.\n",
        "* Predicting help desk staffing requirements based on sales information.\n",
        "* Predicting sales from cross-selling products.\n",
        "* Predicting the impact of discounts on retail sales."
      ],
      "metadata": {
        "id": "rUAfCJ5Jh5xB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2. EXPLANATORY VS. PREDICTIVE MODELING"
      ],
      "metadata": {
        "id": "XBNBFMp6SWuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using linear regression for prediction, it is crucial to distinguish between two popular but different objectives."
      ],
      "metadata": {
        "id": "_T_PwqrxmON5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.1. The Distinction"
      ],
      "metadata": {
        "id": "SgJqc0jymmfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **Explanatory Task :** Explaining or quantifying the average effect of inputs on an outcome (explanatory or descriptive task, respectively)\n",
        "2.   **Predictive Task :** Predicting the outcome value for new records, given their input values\n",
        "\n"
      ],
      "metadata": {
        "id": "g0LHC3J5nTui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.2. The Classical Statistical Approach (Explanatory)"
      ],
      "metadata": {
        "id": "uDXlXIxtmts7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Focus :** Objective #1 (explaning)\n",
        "* **Data view :** Data is treated as a random sample from a larger population. The model attempts to capture the **average relationship** in that population.\n",
        "* **Interpretation :** Used to generate statements like **\"A unit increase in service speed ($X_1$) is associated with an average increase of 5 points in Satisfaction ($Y$), all other factors ($X_2$, $X_3$, ... , $X_p$) being equal. \"**\n",
        "* **Explanatory modeling:** If causal structure is known ($X$ causes $Y$), used for actionable policy changes.\n",
        "* **Descriptive Modeling:** If causal structure is unknown, it quantifies the degree of **association**."
      ],
      "metadata": {
        "id": "53EdVm95n-Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.3. The Predictive Analytics Approach (Data Mining)"
      ],
      "metadata": {
        "id": "7iQo94_6m228"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Focus:** Object #2 (predicting).\n",
        "* **Target:** Predicting **new individual records**\n",
        "* **Mindset:** We are not interested in the coefficients themselves or the **average record**, but rather in the **predictions ($\\hat{y}$)** the model generates.\n",
        "* **Usage:** Used for **micro-decision-making** at the record level (e.g., predicting satisfaction for *each* new customer)."
      ],
      "metadata": {
        "id": "4alQ_-jctjbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.4. Modeling Process & Trade-off"
      ],
      "metadata": {
        "id": "czccsH3um-v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Optmization:**\n",
        "  * *Explanatory:* Tries to fit the **best model to the existing data** to learn underlying relationships.\n",
        "  * *Predictive:* Tries to find a model that best predicts **new individual records**.\n",
        "* **The Overfitting Paradox:** A regression model that fits the existing data **too well** is not likely to perform well with new data.\n",
        "* **Solution:** We look for the model with the highest predictive power by evaluating it on a **holdout set** (validation set) using predictive metrics."
      ],
      "metadata": {
        "id": "49kbB54-uopy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.5. Summary of Key Differences"
      ],
      "metadata": {
        "id": "DmolZAWqnFxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are four main differences in using linear regression for these two scenarios:\n",
        "\n",
        "1. **Definition of \"Good\":**\n",
        "   * *Explanatory :* A good model fits the data closely.\n",
        "   * *Predictive :* A good model predicts new records accurately. (Input variable selection may differ).\n",
        "\n",
        "2. **Data Usage:**\n",
        "   * *Explanatory :* The **entire dataset** is used to estimate the best-fit model (maximizing information).\n",
        "   * *Predictive :* Data is split into a **Training Set** (to estimate the model) and a **Validation/ Holdout Set** (to assess predictive performance on unobserved data).\n",
        "\n",
        "3. **Performance Measures:**\n",
        "   * *Explanatory:* Measures how well the model approximates the data (**Goodness-of-fi**) and the strength of average relationship.\n",
        "   * *Predictive:* Measured by **predictive accuracy**\n",
        "\n",
        "4. **Focus:**\n",
        "   * *Explanatory:* Focus is on the **coefficients ($\\beta$)**.\n",
        "   * *Predictive:* Focus is on the **predictions ($\\hat{y}$)**."
      ],
      "metadata": {
        "id": "3rtA3AiFmiqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For these reasons, it is extremely important to know the goal of the analysis before beginning the modeling process. A good predictive model can have a looser fit to the data on which it is based, and a good explanatory model can have low prediction accuracy. In the remainder of this chapter,we focus on predictive models because these are more popular in data mining and because most statistics textbooks focus on explanatory modeling."
      ],
      "metadata": {
        "id": "elqAtfUszUE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3. ESTIMATING THE REGRESSION EQUATION AND PREDICTION"
      ],
      "metadata": {
        "id": "p143lu77Sco2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.1. Estimation method: Ordinary Least Squares (OLS)"
      ],
      "metadata": {
        "id": "m70P5HOWQ0Y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One predictors and their forms are selected, the coefficients are estimated from the data using a method called **Ordinary Least Squares (OLS)**.\n",
        "\n",
        "* **Objective:** Find values $\\hat{\\beta_0}, \\hat{\\beta_1}, \\dots, \\hat{\\beta_p}$ that **minimize the sum of squared deviations** between the actual outcome values ($Y$) and their predicted values ($\\hat{Y}$).\n",
        "\n",
        "* **Prediction Equation:** To predict the value for a new record, we use the equation:\n",
        "\n",
        "    $$\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1} + \\dots + \\hat{\\beta_p}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "AUJcdJxZRrol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.2. Statistical assumptions"
      ],
      "metadata": {
        "id": "Er6N6evrRC7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For OLS estimates to be the **best** (unbiased and having the smallest mean squared error), the following assumptions are typically made:\n",
        "\n",
        "1. The noise $\\epsilon$ (or equivalently, $Y$) follows a **normal distribution**\n",
        "\n",
        "2. The choice of predictors and their form is correct (*linearity*)\n",
        "\n",
        "3. The records are independent of each other.\n",
        "\n",
        "4. The variability in the outcome values for a given set of predictors is the same regardless of the values of the predictors (*homoskedasticity*)"
      ],
      "metadata": {
        "id": "KSGRYCN3WdAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.3. Data mining perspective: Prediction vs. Assumptions"
      ],
      "metadata": {
        "id": "2oYjG-s5RYWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Key insight:** For the goal of **prediction**, satisfying the strict statistical assumptions (like the normal distribution of noise) is often of **secondary interest**.\n",
        "\n",
        "* **Focus:**: Even if assumptions are violated, predictions can still be sufficiently accurate. The priority is to evaluate the model's **predictive performance** on a validation set rather than just checking assumptions."
      ],
      "metadata": {
        "id": "4GzUd_YrXzda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.4. EXAMPLE : PREDICTING THE PRICE OF USED TOYATA COROLLA CARS"
      ],
      "metadata": {
        "id": "lp88H58MSmsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Goal:** Predict the price of used cars to ensure dealership profitability.\n",
        "* **Data Partitioning:** The dataset (1000 records) is partitioned into a **Training Set (60%)** for fitting the model and a **Validation Set (40%)** for evaluating performance.\n",
        "* **Handling Categorical Predictors (Dummy Variables):**\n",
        "    * Categorical variables like `Fuel Type` (Petrol, Diesel, CNG) must be converted into **dummy variables** (0/1).\n",
        "    * **The $N-1$ Rule:** If a variable has $N$ categories, we create **$N-1$ dummy variables**.\n",
        "    * *Example:* For `Fuel Type` (3 categories), we create `Fuel_Type_Petrol` and `Fuel_Type_Diesel`. The third category (`CNG`) is redundant. Including it would cause the regression to fail due to perfect linear combination.\n"
      ],
      "metadata": {
        "id": "YMmuUbsWYtdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.1. VARIABLES IN THE TOYOTA COROLLA EXAMPLE**"
      ],
      "metadata": {
        "id": "rkKZtdbbSuID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb = pd.read_csv(\"Table_6.1.csv\")\n",
        "tb\n"
      ],
      "metadata": {
        "id": "xsBzYhUW6Xgn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "5cd2fd6d-bf7d-43cb-cfd3-3491b8157464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Variable                           Description\n",
              "0        Price                  Offer price in Euros\n",
              "1          Age       Age in months as of August 2004\n",
              "2   Kilometers    Accumulated kilimeters on odometer\n",
              "3    Fuel type    Fuel type(Petrol or Diesel or CNG)\n",
              "4           HP                            Horsepower\n",
              "5      Metalic     Metalic color?(Yes = 1 or No = 0)\n",
              "6    Automatic            Automatic(Yes =1 or No =0)\n",
              "7           CC  Cylinder volume in cubic centimeters\n",
              "8        Doors                        Numer of doors\n",
              "9      QuarTax           Quarterly road tax in Euros\n",
              "10      Weight                   Weight in kiligrams"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37e6d148-c1fc-49a5-b8fc-d89681d411c6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variable</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Price</td>\n",
              "      <td>Offer price in Euros</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Age</td>\n",
              "      <td>Age in months as of August 2004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kilometers</td>\n",
              "      <td>Accumulated kilimeters on odometer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fuel type</td>\n",
              "      <td>Fuel type(Petrol or Diesel or CNG)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HP</td>\n",
              "      <td>Horsepower</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Metalic</td>\n",
              "      <td>Metalic color?(Yes = 1 or No = 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Automatic</td>\n",
              "      <td>Automatic(Yes =1 or No =0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CC</td>\n",
              "      <td>Cylinder volume in cubic centimeters</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Doors</td>\n",
              "      <td>Numer of doors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>QuarTax</td>\n",
              "      <td>Quarterly road tax in Euros</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Weight</td>\n",
              "      <td>Weight in kiligrams</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37e6d148-c1fc-49a5-b8fc-d89681d411c6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37e6d148-c1fc-49a5-b8fc-d89681d411c6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37e6d148-c1fc-49a5-b8fc-d89681d411c6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e186ecf9-a75c-486c-b9a5-e3434f576e7c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e186ecf9-a75c-486c-b9a5-e3434f576e7c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e186ecf9-a75c-486c-b9a5-e3434f576e7c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3c97fa74-76d2-45c5-a96f-a931039a4755\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tb')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3c97fa74-76d2-45c5-a96f-a931039a4755 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tb');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tb",
              "summary": "{\n  \"name\": \"tb\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Variable\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Metalic\",\n          \"Price\",\n          \"QuarTax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"Metalic color?(Yes = 1 or No = 0)\",\n          \"Offer price in Euros\",\n          \"Quarterly road tax in Euros\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.2. PRICES AND ATTRIBUTES FOR USED TOYOTA COROLLA CARS (SELECTED ROWS AND COLUMNS ONLY)**"
      ],
      "metadata": {
        "id": "PLXVez3wS0wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_df = pd.read_csv('ToyotaCorolla.csv')\n",
        "\n",
        "car_df_needed = ['Price', 'Age_08_04', 'KM', 'Fuel_Type', 'HP', 'Met_Color', 'Automatic', 'cc', 'Doors', 'Quarterly_Tax', 'Weight']\n",
        "car_df = car_df[[c for c in car_df_needed if c in car_df.columns]]\n",
        "car_df = car_df.rename(columns= {'cc':'CC'})\n",
        "car_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "tSJtxaMBGHQP",
        "outputId": "ba2de953-586e-4368-f738-1726a2ca89b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Price  Age_08_04     KM Fuel_Type   HP  Met_Color  Automatic    CC  \\\n",
              "0     13500         23  46986    Diesel   90          1          0  2000   \n",
              "1     13750         23  72937    Diesel   90          1          0  2000   \n",
              "2     13950         24  41711    Diesel   90          1          0  2000   \n",
              "3     14950         26  48000    Diesel   90          0          0  2000   \n",
              "4     13750         30  38500    Diesel   90          0          0  2000   \n",
              "...     ...        ...    ...       ...  ...        ...        ...   ...   \n",
              "1431   7500         69  20544    Petrol   86          1          0  1300   \n",
              "1432  10845         72  19000    Petrol   86          0          0  1300   \n",
              "1433   8500         71  17016    Petrol   86          0          0  1300   \n",
              "1434   7250         70  16916    Petrol   86          1          0  1300   \n",
              "1435   6950         76      1    Petrol  110          0          0  1600   \n",
              "\n",
              "      Doors  Quarterly_Tax  Weight  \n",
              "0         3            210    1165  \n",
              "1         3            210    1165  \n",
              "2         3            210    1165  \n",
              "3         3            210    1165  \n",
              "4         3            210    1170  \n",
              "...     ...            ...     ...  \n",
              "1431      3             69    1025  \n",
              "1432      3             69    1015  \n",
              "1433      3             69    1015  \n",
              "1434      3             69    1015  \n",
              "1435      5             19    1114  \n",
              "\n",
              "[1436 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05700ff8-3f1c-4274-84a6-3afd2b5a8de5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Age_08_04</th>\n",
              "      <th>KM</th>\n",
              "      <th>Fuel_Type</th>\n",
              "      <th>HP</th>\n",
              "      <th>Met_Color</th>\n",
              "      <th>Automatic</th>\n",
              "      <th>CC</th>\n",
              "      <th>Doors</th>\n",
              "      <th>Quarterly_Tax</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13500</td>\n",
              "      <td>23</td>\n",
              "      <td>46986</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>1165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13750</td>\n",
              "      <td>23</td>\n",
              "      <td>72937</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>1165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13950</td>\n",
              "      <td>24</td>\n",
              "      <td>41711</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>1165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14950</td>\n",
              "      <td>26</td>\n",
              "      <td>48000</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>1165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13750</td>\n",
              "      <td>30</td>\n",
              "      <td>38500</td>\n",
              "      <td>Diesel</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>3</td>\n",
              "      <td>210</td>\n",
              "      <td>1170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>7500</td>\n",
              "      <td>69</td>\n",
              "      <td>20544</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1300</td>\n",
              "      <td>3</td>\n",
              "      <td>69</td>\n",
              "      <td>1025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>10845</td>\n",
              "      <td>72</td>\n",
              "      <td>19000</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1300</td>\n",
              "      <td>3</td>\n",
              "      <td>69</td>\n",
              "      <td>1015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>8500</td>\n",
              "      <td>71</td>\n",
              "      <td>17016</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1300</td>\n",
              "      <td>3</td>\n",
              "      <td>69</td>\n",
              "      <td>1015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>7250</td>\n",
              "      <td>70</td>\n",
              "      <td>16916</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1300</td>\n",
              "      <td>3</td>\n",
              "      <td>69</td>\n",
              "      <td>1015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>6950</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>Petrol</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1600</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>1114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1436 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05700ff8-3f1c-4274-84a6-3afd2b5a8de5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-05700ff8-3f1c-4274-84a6-3afd2b5a8de5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-05700ff8-3f1c-4274-84a6-3afd2b5a8de5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fb12bf0e-74b6-4025-8405-613f4ff52ed0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb12bf0e-74b6-4025-8405-613f4ff52ed0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fb12bf0e-74b6-4025-8405-613f4ff52ed0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f518e378-32c5-4f8a-b8e9-b17188eb4a3e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('car_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f518e378-32c5-4f8a-b8e9-b17188eb4a3e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('car_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "car_df",
              "summary": "{\n  \"name\": \"car_df\",\n  \"rows\": 1436,\n  \"fields\": [\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3626,\n        \"min\": 4350,\n        \"max\": 32500,\n        \"num_unique_values\": 236,\n        \"samples\": [\n          17795,\n          10350,\n          8995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age_08_04\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 1,\n        \"max\": 80,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          32,\n          39,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37506,\n        \"min\": 1,\n        \"max\": 243000,\n        \"num_unique_values\": 1263,\n        \"samples\": [\n          57829,\n          51000,\n          34882\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel_Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Diesel\",\n          \"Petrol\",\n          \"CNG\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 69,\n        \"max\": 192,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          107,\n          72,\n          90\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Met_Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Automatic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 424,\n        \"min\": 1300,\n        \"max\": 16000,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          1975,\n          1300\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Doors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quarterly_Tax\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41,\n        \"min\": 19,\n        \"max\": 283,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          72,\n          197\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52,\n        \"min\": 1000,\n        \"max\": 1615,\n        \"num_unique_values\": 59,\n        \"samples\": [\n          1165,\n          1065\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predictive Measures of Error**\n",
        "\n",
        "In predictive modeling, we typically do not use $R^2$ to assess performance on the validation set. Instead, we use measures based on the **prediction error** ($e_i = y_i - \\hat{y}_i$).\n",
        "\n",
        "### **1. Key Error Metrics**\n",
        "\n",
        "| Metric | Name | Formula | Key Characteristics & Usage |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **ME** | **Mean Error** | $$\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$ | • **Purpose:** Measures **Bias**. Tells if the model is over-forecasting or under-forecasting.<br>• **Interpretation:**<br>  - $ME > 0$: Under-forecast (Actual > Predicted).<br>  - $ME < 0$: Over-forecast (Actual < Predicted).<br>• **Note:** $ME \\approx 0$ does not mean the model is good (positive and negative errors cancel out). |\n",
        "| **MAE** | **Mean Absolute Error** | $$\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$ | • **Meaning:** On average, how many units is the prediction off by?<br>• **Usage:** Good for reporting to management (easy to explain). Use when the cost of error increases **linearly**.<br>• **Pros/Cons:** Robust to outliers (does not penalize large errors heavily). |\n",
        "| **RMSE** | **Root Mean Squared Error** | $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$ | • **Meaning:** Standard deviation of the prediction errors.<br>• **Usage:** Use when you want to heavily penalize **large errors** (outliers).<br>• **Pros/Cons:** Very sensitive to outliers. A few bad predictions can inflate RMSE significantly. |\n",
        "| **MAPE** | **Mean Absolute Percentage Error** | $$\\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$ | • **Meaning:** Average error in percentage terms.<br>• **Usage:** Useful for comparing performance across products/channels with **different scales**.<br>• **Note:** Undefined if Actual ($y$) = 0. Can \"explode\" if values are very small. |\n",
        "| **MPE** | **Mean Percentage Error** | $$\\frac{100\\%}{n}\\sum_{i=1}^{n}\\left(\\frac{y_i - \\hat{y}_i}{y_i}\\right)$$ | • **Purpose:** Measures **Percentage Bias**.<br>• **Interpretation:** Similar to ME but in percentage.<br>  - $MPE > 0$: Under-forecast in %.<br>  - $MPE < 0$: Over-forecast in %. |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Strategic Metric Interpretation**\n",
        "How to read these metrics to make decisions?\n",
        "\n",
        "**Step 1: Check for Bias (ME & MPE)**\n",
        "* If $ME \\approx 0$ and $MPE \\approx 0 \\rightarrow$ The model is **unbiased** (predictions are centered around actuals).\n",
        "* If $ME > 0 \\rightarrow$ The model tends to **under-forecast**.\n",
        "* If $ME < 0 \\rightarrow$ The model tends to **over-forecast**.\n",
        "* *Action:* Use this to adjust the intercept or scaling if necessary.\n",
        "\n",
        "**Step 2: Detect Large Errors (RMSE vs. MAE)**\n",
        "* If $RMSE \\approx MAE \\rightarrow$ Errors are distributed evenly. The model is **stable**.\n",
        "* If $RMSE \\gg MAE$ (significantly larger) \\rightarrow **Warning!** There are **large outliers** (big mistakes) in the predictions.\n",
        "* *Insight:* Tells you if there are specific \"disaster\" points causing high error.\n",
        "\n",
        "**Step 3: Assess Relative Accuracy (MAPE)**\n",
        "* $MAPE < 10\\% \\rightarrow$ Excellent.\n",
        "* $10\\% - 20\\% \\rightarrow$ Acceptable.\n",
        "* $> 40\\% \\rightarrow$ Problematic.\n",
        "* *Insight:* Helps determine if the error level is acceptable for the business scale."
      ],
      "metadata": {
        "id": "8QIssRfUJv7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for fitting a regression model\n",
        "\n",
        "# reduce data frame to the top 1000 rows and select columns for regression analysis\n",
        "car_df = car_df.iloc[0:1000]\n",
        "\n",
        "predictors = ['Age_08_04', 'KM', 'Fuel_Type',  'HP', 'Met_Color', 'Automatic', 'CC', 'Doors', 'Quarterly_Tax', 'Weight']\n",
        "outcome = 'Price'\n",
        "\n",
        "# partition data\n",
        "X = pd.get_dummies(car_df[predictors], drop_first= True )\n",
        "y = car_df[outcome]\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size= 0.4, random_state =1)\n",
        "\n",
        "car_lm = LinearRegression()\n",
        "car_lm.fit(train_X, train_y)\n",
        "\n",
        "# print coefficients\n",
        "print(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lm.coef_ } ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yitn1Hpqxdu7",
        "outputId": "96e7441b-1d07-422f-852b-2252166f7a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Predictor  coefficient\n",
            "0          Age_08_04  -140.748761\n",
            "1                 KM    -0.017840\n",
            "2                 HP    36.103419\n",
            "3          Met_Color    84.281830\n",
            "4          Automatic   416.781954\n",
            "5                 CC     0.017737\n",
            "6              Doors   -50.657863\n",
            "7      Quarterly_Tax    13.625325\n",
            "8             Weight    13.038711\n",
            "9   Fuel_Type_Diesel  1066.464681\n",
            "10  Fuel_Type_Petrol  2310.249543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After running the algorithm on the dataset, we obtainted the regression coefficients. The prediction equation for car price ('Price') is formulated as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\widehat{Price} =&-140.75(Age) - 0.018(KM) + 36.10(HP) + 84.28(Met\\_Color)\\\\\n",
        "& + 416.78(Automatic) + 0.018(CC) - 50.66(Doors) + 13.63(Tax) \\\\\n",
        "& + 13.04(Weight) + 1066.46(Diesel) + 2310.25(Petrol)\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "sxtHXnQYed8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.3. LINEAR REGRESSION MODEL OF PRICE VS. CAR ATTRIBUTES**"
      ],
      "metadata": {
        "id": "A9siuoF0S_NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use predict() to make predictiions on a train set\n",
        "car_lm_pred = car_lm.predict(train_X)\n",
        "\n",
        "print('\\n')\n",
        "result = pd.DataFrame({'Predicted': car_lm_pred, 'Actual': train_y, \"Residual\": train_y - car_lm_pred})\n",
        "print(result.head(20))\n",
        "\n",
        "# print performance measures (training data)\n",
        "regressionSummary(train_y,  car_lm.predict(train_X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bSzYjnxBapW",
        "outputId": "81a3c918-18fe-4ed3-b8c7-58b20349537d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "        Predicted  Actual     Residual\n",
            "371  14749.081378   13995  -754.081378\n",
            "45   17790.301531   19000  1209.698469\n",
            "560  11121.976920   10950  -171.976920\n",
            "748   9338.952080    8950  -388.952080\n",
            "419   8805.266673    8950   144.733327\n",
            "604   6074.147930    7900  1825.852070\n",
            "725   7510.189551    8150   639.810449\n",
            "11   20741.629982   19950  -791.629982\n",
            "214  15042.646405   13500 -1542.646405\n",
            "344  14839.422202   12950 -1889.422202\n",
            "700   8710.273398    7999  -711.273398\n",
            "720   9397.613046    8450  -947.613046\n",
            "147  18527.217123   24500  5972.782877\n",
            "299  13634.145578   13750   115.854422\n",
            "283  14360.894045   13950  -410.894045\n",
            "745   7351.873412    8450  1098.126588\n",
            "412  10981.699996    8950 -2031.699996\n",
            "629   8700.514958    7750  -950.514958\n",
            "795   9857.384342    8950  -907.384342\n",
            "741   9770.516208   10450   679.483792\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 0.0000\n",
            "       Root Mean Squared Error (RMSE) : 1400.5823\n",
            "            Mean Absolute Error (MAE) : 1046.9072\n",
            "          Mean Percentage Error (MPE) : -1.0223\n",
            "Mean Absolute Percentage Error (MAPE) : 9.2994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Training Set Evaluation(Run 1)**\n",
        "\n",
        "**Purpose:** \"Internal Health Check\"- To verify if the model has successfully learned patterns from the data.\n",
        "\n",
        "**Applying Strategic Metric Interpretation:**\n",
        "\n",
        "* **Step 1:Check for Bias (ME)**\n",
        "  * $ME = 0.0000$\n",
        "  * **Conclusion:** The model is **Unbiased**. On the data it learned from, the average of predictions perfectly matches the actuals.\n",
        "\n",
        "* **Step 2: Detect Large Errors(RMSE vs. MAE)**\n",
        "  * Comparing $RMSE (1400.58)$ and $MAE(1046.90)$. The RMSE is approximately 33% higher than MAE.\n",
        "  * **Warning:** There is the presence of outliers causing the error metrix to be heavily penalized.\n",
        "  * **Evidence:** Looking at the sample data table, the row **Index 147** has a residual of **5972**. It is these \"extreme\" points that have pulled the RMSE up.\n",
        "\n",
        "* **Step 3: Assess Relative Accuracy (MAPE)**\n",
        "\n",
        "* &MAPE = 9.30\\%&\n",
        "* **Conclusion:** Below 10% is considered **Excellent**. The model has learned the general rules very well."
      ],
      "metadata": {
        "id": "TwQvGMDRgpFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.4. PREDICTED PRICES (AND ERRORS) FOR 20 CARS IN VALIDATION SET AND SUMMARY PREDICTIVE MEASURES FOR ENTIRE VALIDATION SET (CALLED TEST SET IN R)**"
      ],
      "metadata": {
        "id": "W7IA59XTTJEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for fitting a regression model to training set and predicting prices in validation set\n",
        "\n",
        "# Use predict() to make predictiions on a new set\n",
        "car_lm_pred = car_lm.predict(valid_X)\n",
        "\n",
        "result = pd.DataFrame({'Predicted': car_lm_pred, 'Actual': valid_y, \"Residual\": valid_y - car_lm_pred})\n",
        "print(result.head(20))\n",
        "\n",
        "# Print performance measures (validation data)\n",
        "regressionSummary(valid_y, car_lm_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQmsHgOO1QP0",
        "outputId": "a614d10d-a43b-4388-aacd-5060ca2146ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Predicted  Actual     Residual\n",
            "507  10607.333940   11500   892.666060\n",
            "818   9272.705792    8950  -322.705792\n",
            "452  10617.947808   11450   832.052192\n",
            "368  13600.396275   11450 -2150.396275\n",
            "242  12396.694660   11950  -446.694660\n",
            "929   9496.498212    9995   498.501788\n",
            "262  12480.063217   13500  1019.936783\n",
            "810   8834.146068    7950  -884.146068\n",
            "318  12183.361282    9900 -2283.361282\n",
            "49   19206.965683   21950  2743.034317\n",
            "446  10987.498309   11950   962.501691\n",
            "142  18501.527375   19950  1448.472625\n",
            "968   9914.690947    9950    35.309053\n",
            "345  13827.299932   14950  1122.700068\n",
            "971   7966.732543   10495  2528.267457\n",
            "133  17185.242041   15950 -1235.242041\n",
            "104  19952.658062   19450  -502.658062\n",
            "6    16570.609280   16900   329.390720\n",
            "600  13739.409113   11250 -2489.409113\n",
            "496  11267.513740   11750   482.486260\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 103.6803\n",
            "       Root Mean Squared Error (RMSE) : 1312.8523\n",
            "            Mean Absolute Error (MAE) : 1017.5972\n",
            "          Mean Percentage Error (MPE) : -0.2633\n",
            "Mean Absolute Percentage Error (MAPE) : 9.0111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation set Evaluation(Run 2)**\n",
        "\n",
        "**Purpose:** Real-world performance commitment - Assessing predictive ability on unseen cars.\n",
        "\n",
        "* **Applying Strategic Metric Interpretation:**\n",
        "\n",
        "* **Step 1: Check for Bias (ME)**\n",
        "  * $ME = 103.68$ (Positive)\n",
        "  * **Conclusion:** There is a slight tendency to **Under-forecast**. On average, the model is pricing cars about $104 lower than reality.\n",
        "\n",
        "* **Step 2: Detect Large Errors (RMSE vs. MAE)**\n",
        "  * $RMSE(1312$ is still larger than &MAE(1017)&, but the gap has narrowed compared to the training set.\n",
        "  * **Evidence:** The largest errors (such as **Index 49: 2743**, **Inde 600: -2489**) are better controlled compared to the error of 5972 found in the training set.\n",
        "\n",
        "* **Step 3: Assess Relative Accuracy (MAPE)**\n",
        "\n",
        "* $MAPE = 9.01\\%$\n",
        "* **Conclusion:** It remains at an **Excellent** level,"
      ],
      "metadata": {
        "id": "ytmBiO6oj_q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison & Conclusion (The Critical Decision )**\n",
        "\n",
        "**Question:** Which metrics to use for what?\n",
        "\n",
        "* **Training Stats:** Used for **Health check** (Model Diagnostics).\n",
        "  * If errors here are high $\\rightarrow$ Bad model (Underfitting)\n",
        "  * If errors are extremely low but validation errors are high $\\rightarrow$ Rote Learning (Overfitting).\n",
        "\n",
        "* **Validation Stats**\n",
        "  * Used for **Predictive Reporting**. This is the number you use to answer the client's question. \"How much does this model deviate?\"\n",
        "\n",
        "**Reality Comparison:**\n",
        "* Training RMSE: $1400.5823$\n",
        "* Validation RMSE: $1312.8523$\n",
        "* **Insight:** This is quite interesting and positive. Typically, validation errors are higher. However, here the validation error is lower. This proves the model is **Robust**, not overfitting, and implies the training set contained some noise (like the Inde 147 case) that the validation set did not.\n",
        "\n",
        "**$\\rightarrow$ FINAL CONCLUSION:** Should we use this model to predict car prices? **YES.** The model achieves excellent accuracy ($MAPE \\approx 9\\%$) and demonstrates high stability on new data."
      ],
      "metadata": {
        "id": "uzU0THdTlp3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIGURE 6.1. HISTOGRAM OF MODEL ERRORS (BASED ON VALIDATION SET)**"
      ],
      "metadata": {
        "id": "IBHetsl0TYYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for plotting histogram of validation errors\n",
        "\n",
        "car_lm_pred = car_lm.predict(valid_X)\n",
        "all_residuals = valid_y  - car_lm_pred\n",
        "\n",
        "# Determine the percentage of datapoints with a residual in [-1406, 1406] = approx\n",
        "# 75%\n",
        "print(len(all_residuals[(all_residuals > -1406) & (all_residuals < 1406) ]) / len(all_residuals))\n",
        "\n",
        "pd.DataFrame({\"Residuals\": all_residuals}).hist(bins = 25)\n",
        "plt.figure(figsize=(4, 6))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "x0gOutmX2-U3",
        "outputId": "84786603-0a33-48da-875d-63f3de88e9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7425\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGzCAYAAACPa3XZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKH5JREFUeJzt3X90VOWdx/FPQpJJAiQhARJ+JIKAREHKERqIIgIGUkSUghXF3QqLVNcgP2KpZI9KsHoCbA+4xYDUxbC7bsoWeqogFMiGCqc2KISigJoFCgJCQtHNhB9mMibP/tFlYEzADJk8Mwnv1zlz8D73mfs8M9+Zycc7984NMcYYAQAAWBIa6AkAAIAbC+EDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhA4Df5ObmKiQkpFF9Q0JClJub26zzGTFihEaMGNGsYwDwHeEDaKXWrFmjkJAQzy0sLEzdunXT1KlT9cUXXwR6egBuYGGBngCA5vXSSy+pZ8+eqq6u1q5du7RmzRr98Y9/1IEDBxQZGenXsZ5//nnNnz/fr9sE0PoQPoBWbuzYsRo8eLAk6YknnlDHjh21ePFibdiwQQ8//LBfxwoLC1NYGB8rAK6Nr12AG8zdd98tSTpy5Iin7bPPPtNDDz2k+Ph4RUZGavDgwdqwYYPX/dxutxYuXKg+ffooMjJSCQkJGjZsmIqKijx9Gjrmw+Vyae7cuerUqZPat2+vBx54QCdPnqw3r6lTp6pHjx712hvaZkFBgUaNGqXOnTvL4XDotttu08qVKxv1+JcvX65+/fopOjpaHTp00ODBg1VYWNio+wLwD/4XBbjBHDt2TJLUoUMHSdLBgwd11113qVu3bpo/f77atm2r3/zmN5owYYJ++9vf6oc//KGkv4WAvLw8PfHEE0pLS1NVVZX27NmjvXv3avTo0Vcd74knntBbb72lKVOm6M4779T27ds1bty4Jj2GlStXql+/fnrggQcUFhamjRs36umnn1ZdXZ2ysrKuer833nhDs2bN0kMPPaTZs2erurpaH3/8sT744ANNmTKlSXMC0HiED6CVczqdOnv2rKqrq/XBBx9o4cKFcjgcuv/++yVJs2fPVkpKinbv3i2HwyFJevrppzVs2DA999xznvCxadMm3XffffrVr37V6LE/+ugjvfXWW3r66aeVn58vScrKytJjjz2mjz/++Lof044dOxQVFeVZnjlzpn7wgx9o6dKl1wwfmzZtUr9+/bRu3brrHhtA0/G1C9DKZWRkqFOnTkpOTtZDDz2ktm3basOGDerevbu++uorbd++XQ8//LDOnTuns2fP6uzZs/ryyy+VmZmpQ4cOec6MiYuL08GDB3Xo0KFGj71582ZJ0qxZs7za58yZ06THdGXwuBSu7rnnHv3lL3+R0+m86v3i4uJ08uRJ7d69u0njA2gawgfQyuXn56uoqEjr16/Xfffdp7Nnz3r2cBw+fFjGGL3wwgvq1KmT123BggWSpDNnzkj621kzlZWVuuWWW3T77bdr3rx537n34vPPP1doaKh69erl1d63b98mPab3339fGRkZatu2reLi4tSpUyf90z/9kyRdM3w899xzateundLS0tSnTx9lZWXp/fffb9JcAPiOr12AVi4tLc1ztsuECRM0bNgwTZkyRWVlZaqrq5Mk/fSnP1VmZmaD9+/du7ckafjw4Tpy5Ijeeecdbdu2Tf/6r/+qZcuW6fXXX9cTTzzR5Hle7cfJamtrvZaPHDmie++9V6mpqVq6dKmSk5MVERGhzZs3a9myZZ7H1JBbb71VZWVlevfdd7Vlyxb99re/1YoVK/Tiiy9q4cKFTX4MABqH8AHcQNq0aaO8vDyNHDlSr732mv7hH/5BkhQeHq6MjIzvvH98fLymTZumadOm6fz58xo+fLhyc3OvGj5uuukm1dXV6ciRI157O8rKyur17dChgyorK+u1f/75517LGzdulMvl0oYNG5SSkuJp/8Mf/vCd85ektm3bavLkyZo8ebJqamo0ceJEvfLKK8rJyfH7754AaBhfuwA3mBEjRigtLU2vvvqqYmJiNGLECK1atUqnT5+u1/evf/2r57+//PJLr3Xt2rVT79695XK5rjrW2LFjJUm//OUvvdpfffXVen179eolp9Pp9VXO6dOn9bvf/c6rX5s2bSRJxhhPm9PpVEFBwVXncbXHEBERodtuu03GGLnd7u+8PwD/YM8HcAOaN2+efvSjH2nNmjXKz8/XsGHDdPvtt2vGjBm6+eabVVFRoZKSEp08eVIfffSRJOm2227TiBEjNGjQIMXHx2vPnj1av369Zs6cedVxBg4cqEcffVQrVqyQ0+nUnXfeqeLiYh0+fLhe30ceecRzds2sWbN08eJFrVy5Urfccov27t3r6TdmzBhFRERo/PjxevLJJ3X+/Hm98cYb6ty5c4MB6kpjxoxRUlKS7rrrLiUmJurTTz/Va6+9pnHjxql9+/bX+WwC8JkB0CoVFBQYSWb37t311tXW1ppevXqZXr16mW+++cYcOXLE/PjHPzZJSUkmPDzcdOvWzdx///1m/fr1nvu8/PLLJi0tzcTFxZmoqCiTmppqXnnlFVNTU+Pps2DBAvPtj5Wvv/7azJo1yyQkJJi2bdua8ePHmxMnThhJZsGCBV59t23bZvr3728iIiJM3759zVtvvdXgNjds2GAGDBhgIiMjTY8ePczixYvNm2++aSSZo0ePevrdc8895p577vEsr1q1ygwfPtwkJCQYh8NhevXqZebNm2ecTud1PMMArleIMVfsuwQAAGhmHPMBAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKt8+pGx3Nzcetc/6Nu3rz777DNJUnV1tZ599lmtXbtWLpdLmZmZWrFihRITExs9Rl1dnU6dOqX27dtf9VoPAAAguBhjdO7cOXXt2lWhodfet+HzL5z269dP//3f/315A2GXNzF37lxt2rRJ69atU2xsrGbOnKmJEyf6dNXIU6dOKTk52ddpAQCAIHDixAl17979mn18Dh9hYWFKSkqq1+50OrV69WoVFhZq1KhRkqSCggLdeuut2rVrl4YOHdqo7V/6ieMTJ04oJibG1+k1idvt1rZt2zRmzBiFh4dbHRveqEVwoR7Bg1oEF+pxWVVVlZKTkxt1qQKfw8ehQ4fUtWtXRUZGKj09XXl5eUpJSVFpaancbrfXlTFTU1OVkpKikpKSq4YPl8vldWGqc+fOSZKioqIUFRXl6/SaJCwsTNHR0YqKirrhX0SBRi2CC/UIHtQiuFCPyy5dnLExh0z4FD6GDBmiNWvWqG/fvjp9+rQWLlyou+++WwcOHFB5ebkiIiIUFxfndZ/ExESVl5dfdZt5eXn1jiORpG3btik6OtqX6flNUVFRQMZFfdQiuFCP4EEtggv1kC5evNjovk26tktlZaVuuukmLV26VFFRUZo2bVq9y2unpaVp5MiRWrx4cYPb+Paej0u7bc6ePRuQr12Kioo0evToGz7BBhq1CC7UI3hQi+BCPS6rqqpSx44d5XQ6v/Pvt89fu1wpLi5Ot9xyiw4fPqzRo0erpqZGlZWVXns/KioqGjxG5BKHwyGHw1GvPTw8PGCFDOTY8EYtggv1CB7UIrhQD/n0+Jv0Ox/nz5/XkSNH1KVLFw0aNEjh4eEqLi72rC8rK9Px48eVnp7elGEAAEAr4tOej5/+9KcaP368brrpJp06dUoLFixQmzZt9Oijjyo2NlbTp09Xdna24uPjFRMTo2eeeUbp6emNPtMFAAC0fj6Fj5MnT+rRRx/Vl19+qU6dOmnYsGHatWuXOnXqJElatmyZQkNDNWnSJK8fGQMAALjEp/Cxdu3aa66PjIxUfn6+8vPzmzQpAADQenFtFwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWNenaLgACq8f8TX7ZzrFF4/yyHQBoDPZ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAqrBATwBA4PWYv+ma6x1tjJakSf1zt8pVG9Ls8zm2aFyzjwEgcNjzAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwKC/QEAODbeszf5JftHFs0zi/bAeBf7PkAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFY1KXwsWrRIISEhmjNnjqeturpaWVlZSkhIULt27TRp0iRVVFQ0dZ4AAKCVuO7wsXv3bq1atUoDBgzwap87d642btyodevWaceOHTp16pQmTpzY5IkCAIDW4brCx/nz5/XYY4/pjTfeUIcOHTztTqdTq1ev1tKlSzVq1CgNGjRIBQUF+tOf/qRdu3b5bdIAAKDlCrueO2VlZWncuHHKyMjQyy+/7GkvLS2V2+1WRkaGpy01NVUpKSkqKSnR0KFD623L5XLJ5XJ5lquqqiRJbrdbbrf7eqZ33S6NZ3tc1EctGsfRxtgZJ9R4/dtStMbXD++N4EI9LvPlOfA5fKxdu1Z79+7V7t27660rLy9XRESE4uLivNoTExNVXl7e4Pby8vK0cOHCeu3btm1TdHS0r9Pzi6KiooCMi/qoxbUtSbM73s8H19kdsIk2b94c6Ck0G94bwYV6SBcvXmx0X5/Cx4kTJzR79mwVFRUpMjLS54k1JCcnR9nZ2Z7lqqoqJScna8yYMYqJifHLGI3ldrtVVFSk0aNHKzw83OrY8EYtGqd/7lYr4zhCjX4+uE4v7AmVqy7Eypj+cCA3M9BT8DveG8GFelx26ZuLxvApfJSWlurMmTO64447PG21tbXauXOnXnvtNW3dulU1NTWqrKz02vtRUVGhpKSkBrfpcDjkcDjqtYeHhweskIEcG96oxbW5au0GAVddiPUxm6I1v3Z4bwQX6uHb+82n8HHvvfdq//79Xm3Tpk1TamqqnnvuOSUnJys8PFzFxcWaNGmSJKmsrEzHjx9Xenq6L0MBAIBWyqfw0b59e/Xv39+rrW3btkpISPC0T58+XdnZ2YqPj1dMTIyeeeYZpaenN3iwKQAAuPFc19ku17Js2TKFhoZq0qRJcrlcyszM1IoVK/w9DAAAaKGaHD7ee+89r+XIyEjl5+crPz+/qZsGAACtkN/3fAD4bj3mbwr0FAAgYLiwHAAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwKC/QEACDY9Zi/yS/bObZonF+2A7R07PkAAABWET4AAIBVPoWPlStXasCAAYqJiVFMTIzS09P1+9//3rO+urpaWVlZSkhIULt27TRp0iRVVFT4fdIAAKDl8il8dO/eXYsWLVJpaan27NmjUaNG6cEHH9TBgwclSXPnztXGjRu1bt067dixQ6dOndLEiRObZeIAAKBl8umA0/Hjx3stv/LKK1q5cqV27dql7t27a/Xq1SosLNSoUaMkSQUFBbr11lu1a9cuDR061H+zBgAALdZ1n+1SW1urdevW6cKFC0pPT1dpaancbrcyMjI8fVJTU5WSkqKSkpKrhg+XyyWXy+VZrqqqkiS53W653e7rnd51uTSe7XFRX2uvhaONCfQUfOIINV7/thT+ev34q17+mE9rf2+0NNTjMl+egxBjjE/vqv379ys9PV3V1dVq166dCgsLdd9996mwsFDTpk3zChKSlJaWppEjR2rx4sUNbi83N1cLFy6s115YWKjo6GhfpgYAAALk4sWLmjJlipxOp2JiYq7Z1+c9H3379tW+ffvkdDq1fv16Pf7449qxY8d1TzYnJ0fZ2dme5aqqKiUnJ2vMmDHfOXl/c7vdKioq0ujRoxUeHm51bHhr7bXon7s10FPwiSPU6OeD6/TCnlC56kICPZ1GO5Cb6Zft+Kte/phPa39vtDTU47JL31w0hs/hIyIiQr1795YkDRo0SLt379a//Mu/aPLkyaqpqVFlZaXi4uI8/SsqKpSUlHTV7TkcDjkcjnrt4eHhAStkIMeGt9ZaC1dty/kDfiVXXUiLmru/Xjv+esz+fC231vdGS0U9fHt9N/l3Purq6uRyuTRo0CCFh4eruLjYs66srEzHjx9Xenp6U4cBAACthE97PnJycjR27FilpKTo3LlzKiws1HvvvaetW7cqNjZW06dPV3Z2tuLj4xUTE6NnnnlG6enpnOkCAAA8fAofZ86c0Y9//GOdPn1asbGxGjBggLZu3arRo0dLkpYtW6bQ0FBNmjRJLpdLmZmZWrFiRbNMHAAAtEw+hY/Vq1dfc31kZKTy8/OVn5/fpEkBAIDWi2u7AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKzy+cJyANBS9Ji/KdBTANAA9nwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMCqsEBPAGhJuEQ7ADQdez4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjlU/jIy8vT97//fbVv316dO3fWhAkTVFZW5tWnurpaWVlZSkhIULt27TRp0iRVVFT4ddIAAKDl8il87NixQ1lZWdq1a5eKiorkdrs1ZswYXbhwwdNn7ty52rhxo9atW6cdO3bo1KlTmjhxot8nDgAAWqYwXzpv2bLFa3nNmjXq3LmzSktLNXz4cDmdTq1evVqFhYUaNWqUJKmgoEC33nqrdu3apaFDh/pv5gAAoEXyKXx8m9PplCTFx8dLkkpLS+V2u5WRkeHpk5qaqpSUFJWUlDQYPlwul1wul2e5qqpKkuR2u+V2u5syPZ9dGs/2uKgvWGvhaGMCPYWAcIQar39xffzxeg7W98aNinpc5stzEGKMua5Pk7q6Oj3wwAOqrKzUH//4R0lSYWGhpk2b5hUmJCktLU0jR47U4sWL620nNzdXCxcurNdeWFio6Ojo65kaAACw7OLFi5oyZYqcTqdiYmKu2fe693xkZWXpwIEDnuBxvXJycpSdne1ZrqqqUnJyssaMGfOdk/c3t9utoqIijR49WuHh4VbHhrdgrUX/3K2BnkJAOEKNfj64Ti/sCZWrLiTQ02mxDuRmNnkbwfreuFFRj8sufXPRGNcVPmbOnKl3331XO3fuVPfu3T3tSUlJqqmpUWVlpeLi4jztFRUVSkpKanBbDodDDoejXnt4eHjAChnIseEt2Grhqr2x//C66kJu+OegKfz5Wg6298aNjnr49vr26WwXY4xmzpyp3/3ud9q+fbt69uzptX7QoEEKDw9XcXGxp62srEzHjx9Xenq6L0MBAIBWyqc9H1lZWSosLNQ777yj9u3bq7y8XJIUGxurqKgoxcbGavr06crOzlZ8fLxiYmL0zDPPKD09nTNdAACAJB/Dx8qVKyVJI0aM8GovKCjQ1KlTJUnLli1TaGioJk2aJJfLpczMTK1YscIvkwUAAC2fT+GjMSfGREZGKj8/X/n5+dc9KQAA0HpxbRcAAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFaFBXoCgA095m8K9BQAAP+PPR8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIpTbQHAEn+c8u1oY7QkzQ+TAQKIPR8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKvCAj0B4Fr6526VqzYk0NMAAPgRez4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABglc/hY+fOnRo/fry6du2qkJAQvf32217rjTF68cUX1aVLF0VFRSkjI0OHDh3y13wBAEAL53P4uHDhgr73ve8pPz+/wfVLlizRL3/5S73++uv64IMP1LZtW2VmZqq6urrJkwUAAC2fzxeWGzt2rMaOHdvgOmOMXn31VT3//PN68MEHJUn//u//rsTERL399tt65JFHmjZbAADQ4vn1qrZHjx5VeXm5MjIyPG2xsbEaMmSISkpKGgwfLpdLLpfLs1xVVSVJcrvdcrvd/pzed7o0nu1xUd+lGjhCTYBnAulyHahH4F2qAZ9TwYG/G5f58hz4NXyUl5dLkhITE73aExMTPeu+LS8vTwsXLqzXvm3bNkVHR/tzeo1WVFQUkHFR388H1wV6CrgC9QgefE4FF+ohXbx4sdF9/Ro+rkdOTo6ys7M9y1VVVUpOTtaYMWMUExNjdS5ut1tFRUUaPXq0wsPDrY7d2vTP3dqk+ztCjX4+uE4v7AmVqy7ET7PC9aIeweNSLficCg783bjs0jcXjeHX8JGUlCRJqqioUJcuXTztFRUVGjhwYIP3cTgccjgc9drDw8MDVshAjt1auGr98wfKVRfit22h6ahH8OBzKrhQD/n0+P36Ox89e/ZUUlKSiouLPW1VVVX64IMPlJ6e7s+hAABAC+Xzno/z58/r8OHDnuWjR49q3759io+PV0pKiubMmaOXX35Zffr0Uc+ePfXCCy+oa9eumjBhgj/nDQAAWiifw8eePXs0cuRIz/Kl4zUef/xxrVmzRj/72c904cIF/eQnP1FlZaWGDRumLVu2KDIy0n+zBgAALZbP4WPEiBEy5uqn24WEhOill17SSy+91KSJAQCA1ingZ7sAAHzXP3erXw7+PbZonB9mA/iGC8sBAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq8ICPQEElx7zNwV6CgCAVo49HwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACs4nc+AABN5q/fCDq2aJxftoPgxp4PAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjFqbathL9OcwNwY+GzA4HAng8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFgVFugJ2Haty0c72hgtSZP6526Vqzbkmts5tmicv6cGADe8a31G+6K1fka3lueHPR8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsOqGO9XWX/x1uhMAwP9sfUY39icaAn1qa7BhzwcAALCK8AEAAKxqtvCRn5+vHj16KDIyUkOGDNGHH37YXEMBAIAWpFnCx3/9138pOztbCxYs0N69e/W9731PmZmZOnPmTHMMBwAAWpBmCR9Lly7VjBkzNG3aNN122216/fXXFR0drTfffLM5hgMAAC2I3892qampUWlpqXJycjxtoaGhysjIUElJSb3+LpdLLpfLs+x0OiVJX331ldxut7+np7BvLlx9XZ3RxYt1CnOHqrbu2heWQ/OiFsGFegQPahFcGluPL7/80j/jXeNvmC/8NZ8rnTt3TpJkjPnuzsbPvvjiCyPJ/OlPf/JqnzdvnklLS6vXf8GCBUYSN27cuHHjxq0V3E6cOPGdWSHgv/ORk5Oj7Oxsz3JdXZ2++uorJSQkKCTEbqqvqqpScnKyTpw4oZiYGKtjwxu1CC7UI3hQi+BCPS4zxujcuXPq2rXrd/b1e/jo2LGj2rRpo4qKCq/2iooKJSUl1evvcDjkcDi82uLi4vw9LZ/ExMTc8C+iYEEtggv1CB7UIrhQj7+JjY1tVD+/H3AaERGhQYMGqbi42NNWV1en4uJipaen+3s4AADQwjTL1y7Z2dl6/PHHNXjwYKWlpenVV1/VhQsXNG3atOYYDgAAtCDNEj4mT56sv/71r3rxxRdVXl6ugQMHasuWLUpMTGyO4fzG4XBowYIF9b4Ggn3UIrhQj+BBLYIL9bg+IcY05pwYAAAA/+DaLgAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAqhsqfLhcLg0cOFAhISHat2+f17qPP/5Yd999tyIjI5WcnKwlS5bUu/+6deuUmpqqyMhI3X777dq8ebPXemOMXnzxRXXp0kVRUVHKyMjQoUOHmvMhtSjHjh3T9OnT1bNnT0VFRalXr15asGCBampqvPpRi+CSn5+vHj16KDIyUkOGDNGHH34Y6Cm1eHl5efr+97+v9u3bq3PnzpowYYLKysq8+lRXVysrK0sJCQlq166dJk2aVO+Xo48fP65x48YpOjpanTt31rx58/TNN9949Xnvvfd0xx13yOFwqHfv3lqzZk1zP7wWbdGiRQoJCdGcOXM8bdSiGfjjYnItxaxZs8zYsWONJPPnP//Z0+50Ok1iYqJ57LHHzIEDB8yvf/1rExUVZVatWuXp8/7775s2bdqYJUuWmE8++cQ8//zzJjw83Ozfv9/TZ9GiRSY2Nta8/fbb5qOPPjIPPPCA6dmzp/n6669tPsyg9fvf/95MnTrVbN261Rw5csS88847pnPnzubZZ5/19KEWwWXt2rUmIiLCvPnmm+bgwYNmxowZJi4uzlRUVAR6ai1aZmamKSgoMAcOHDD79u0z9913n0lJSTHnz5/39HnqqadMcnKyKS4uNnv27DFDhw41d955p2f9N998Y/r3728yMjLMn//8Z7N582bTsWNHk5OT4+nzl7/8xURHR5vs7GzzySefmOXLl5s2bdqYLVu2WH28LcWHH35oevToYQYMGGBmz57taacW/nfDhI/Nmzeb1NRUc/DgwXrhY8WKFaZDhw7G5XJ52p577jnTt29fz/LDDz9sxo0b57XNIUOGmCeffNIYY0xdXZ1JSkoy//zP/+xZX1lZaRwOh/n1r3/dTI+q5VuyZInp2bOnZ5laBJe0tDSTlZXlWa6trTVdu3Y1eXl5AZxV63PmzBkjyezYscMY87fXa3h4uFm3bp2nz6effmokmZKSEmPM3z7TQkNDTXl5uafPypUrTUxMjOf987Of/cz069fPa6zJkyebzMzM5n5ILc65c+dMnz59TFFRkbnnnns84YNaNI8b4muXiooKzZgxQ//xH/+h6OjoeutLSko0fPhwRUREeNoyMzNVVlam//3f//X0ycjI8LpfZmamSkpKJElHjx5VeXm5V5/Y2FgNGTLE0wf1OZ1OxcfHe5apRfCoqalRaWmp1/MYGhqqjIwMnkc/czqdkuR5L5SWlsrtdns996mpqUpJSfE89yUlJbr99tu9fjk6MzNTVVVVOnjwoKfPtd4ruCwrK0vjxo2r93xRi+bR6sOHMUZTp07VU089pcGDBzfYp7y8vN5Pv19aLi8vv2afK9dfeb+G+sDb4cOHtXz5cj355JOeNmoRPM6ePava2lqex2ZWV1enOXPm6K677lL//v0l/e01HBERUe8K399+nV/ve6Wqqkpff/11czycFmnt2rXau3ev8vLy6q2jFs2jxYaP+fPnKyQk5Jq3zz77TMuXL9e5c+eUk5MT6Cm3Wo2txZW++OIL/eAHP9CPfvQjzZgxI0AzBwIvKytLBw4c0Nq1awM9lRvSiRMnNHv2bP3nf/6nIiMjAz2dG0azXFjOhmeffVZTp069Zp+bb75Z27dvV0lJSb2L/gwePFiPPfaY/u3f/k1JSUn1jly+tJyUlOT5t6E+V66/1NalSxevPgMHDvT58bUkja3FJadOndLIkSN155136le/+pVXP2oRPDp27Kg2bdpc87lG08ycOVPvvvuudu7cqe7du3vak5KSVFNTo8rKSq//4/726/zbZx419r0SExOjqKio5nhILU5paanOnDmjO+64w9NWW1urnTt36rXXXtPWrVupRXMI9EEnze3zzz83+/fv99y2bt1qJJn169ebEydOGGMuH+RYU1PjuV9OTk69gxzvv/9+r22np6fXO8jxF7/4hWe90+nkIMdvOXnypOnTp4955JFHzDfffFNvPbUILmlpaWbmzJme5draWtOtWzcOOG2iuro6k5WVZbp27Wr+53/+p976Swc5rl+/3tP22WefNXiQ45VnHq1atcrExMSY6upqY8zfDnLs37+/17YfffTRG/Ygx4ZUVVV5/Y3Yv3+/GTx4sPm7v/s7s3//fmrRTFp9+Pi2o0eP1jvbpbKy0iQmJpq///u/NwcOHDBr16410dHR9U7vDAsLM7/4xS/Mp59+ahYsWNDg6Z1xcXHmnXfeMR9//LF58MEHOb3zCidPnjS9e/c29957rzl58qQ5ffq053YJtQgua9euNQ6Hw6xZs8Z88skn5ic/+YmJi4vzOqofvvvHf/xHExsba9577z2v98HFixc9fZ566imTkpJitm/fbvbs2WPS09NNenq6Z/2l0zvHjBlj9u3bZ7Zs2WI6derU4Omd8+bNM59++qnJz8+/oU/vbKwrz3Yxhlo0B8LH//voo4/MsGHDjMPhMN26dTOLFi2qd9/f/OY35pZbbjERERGmX79+ZtOmTV7r6+rqzAsvvGASExONw+Ew9957rykrK2vOh9OiFBQUGEkN3q5ELYLL8uXLTUpKiomIiDBpaWlm165dgZ5Si3e190FBQYGnz9dff22efvpp06FDBxMdHW1++MMfegV1Y4w5duyYGTt2rImKijIdO3Y0zz77rHG73V59/vCHP5iBAweaiIgIc/PNN3uNgYZ9O3xQC/8LMcYY69/1AACAG1aLPdsFAAC0TIQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWPV/Obg8Oj7FGFIAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** How to choose the \"optimal range\" for plotting?\n",
        "\n",
        "Looking at the `Residual` column in the validation set:\n",
        "* Most small errors cluster around 0 (e.g., 892, -322, 832).\n",
        "* The largest errors are around $\\pm 2500$ (e.g., 2743, -2489).\n",
        "\n",
        "\n",
        "**Binning Strategy:**\n",
        "1.  **Center:** Must always be **0**. The Peak of the bell curve must be located here.\n",
        "2.  **Range:** Choose a range that captures 95% of the data.\n",
        "    * Based on the data table, most cars deviate within the range of **$\\pm 2000$**.\n",
        "    * Therefore, we should set the X-axis from **-2000 to +2000**.\n",
        "    * Points falling outside this range (like car **Index 49**) will be considered **Outliers** in the tails of the chart. This helps the histogram focus on the \"majority\" and clearly display the Normal Distribution shape (Bell curve).\n"
      ],
      "metadata": {
        "id": "EtDGBpr1nyYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.4. VARIABLE SELECTION IN LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "7VFkjt6tTfQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A frequent problem in data mining is using a regression equation to predict the value of a dependent variable when there are too many variables available to choose as predictors.\n",
        "\n",
        "Given the high processing speeds of modern algorithms for multiple regression calculations, analysts are often tempted by the **\"kitchen-sink approach\"**: Why bother selecting a subset? Just throw all variables into the model. However, there are several reasons to exercise caution before including every possible variable in a model:\n",
        "\n",
        "1.  **Cost and Feasibility:** It might be expensive or unfeasible to collect the full set of predictors for future predictions.\n",
        "2.  **Accuracy:** We might be able to measure fewer predictors more accurately (e.g., in surveys).\n",
        "3.  **Missing Values:** The more predictors used, the higher the chance of missing data. If we delete or impute missing records, having many predictors leads to a higher rate of record deletion/imputation.\n",
        "4.  **Parsimony:** This is a crucial property of good models. We gain more insight into the influence of predictors in models with fewer parameters.\n",
        "5.  **Multicollinearity:** Regression coefficient estimates are likely to be unstable due to multicollinearity (the presence of two or more predictors sharing the same linear relationship). A rough rule of thumb is that the number of records $n$ should be greater than $5(p + 2)$, where $p$ is the number of predictors.\n",
        "6.  **Bias-Variance Trade-off:**\n",
        "    * Using predictors that are uncorrelated with the outcome variable will **increase the variance** of predictions.\n",
        "    * Dropping predictors that are actually correlated with the outcome variable can **increase the average error (bias)** of predictions.\n",
        "\n",
        "This trade-off is particularly important when dealing with a large number of predictors. Removing \"noise\" variables (those with small coefficients relative to standard deviation and correlated with others) improves prediction because it reduces prediction variance. Therefore, methods to reduce the number of predictors $p$ to a smaller subset are frequently used.\n",
        "\n"
      ],
      "metadata": {
        "id": "HTNr3rjsoEv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REDUCING THE NUMBER OF PREDICTORS"
      ],
      "metadata": {
        "id": "SqBWN4b5TlHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HOW TO REDUCE THE NUMBER OF PREDICTORS"
      ],
      "metadata": {
        "id": "nehhJ8r5Tp6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Use Domain Knowledge**\n",
        "The first step is always to leverage domain expertise. It is essential to understand what the predictors are measuring and why they are related to the outcome variable. Practical reasons to remove a predictor include high collection costs, inaccuracy, high correlation with another predictor, or excessive missing values. Useful tools include frequency tables, correlation matrices, and summary plots.\n",
        "\n",
        "**Step 2: Use Computational Methods**\n",
        "There are two general types of algorithmic methods for variable reduction:\n",
        "1.  **Exhaustive Search:** Finding the \"best\" subset by fitting regression models with all possible combinations of predictors.\n",
        "2.  **Partial Search (Iterative):** Searching through a partial set of models (e.g., Stepwise Regression).\n"
      ],
      "metadata": {
        "id": "LAN4reVoog8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1: Exhaustive Search\n",
        "\n",
        "The idea here is to evaluate all subsets of predictors. To select a model that is neither too simple (under-fit) nor too complex (over-fit), we use the following evaluation criteria:\n",
        "\n",
        "**1. Adjusted $R^2$ ($R^2_{adj}$)**\n",
        "Defined as:\n",
        "$$R^2_{adj} = 1 - \\frac{n - 1}{n - p - 1} (1 - R^2)$$\n",
        "* Where $R^2$ is the proportion of variation explained by the model.\n",
        "* Unlike standard $R^2$, $R^2_{adj}$ imposes a **penalty** on the number of predictors used. This prevents the artificial inflation of $R^2$ that occurs when simply adding non-informative variables.\n",
        "* Selecting the subset with the highest $R^2_{adj}$ is equivalent to selecting the subset that minimizes the RMSE on the training set.\n",
        "\n",
        "**2. Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)**\n",
        "These metrics measure goodness of fit but include a heavier penalty for model complexity (number of parameters):\n",
        "\n",
        "$$AIC = n \\ln(SSE/n) + n(1 + \\ln(2\\pi)) + 2(p + 1)$$\n",
        "\n",
        "$$BIC = n \\ln(SSE/n) + n(1 + \\ln(2\\pi)) + \\ln(n)(p + 1)$$\n",
        "\n",
        "* Where $SSE$ is the Sum of Squared Errors.\n",
        "* Generally, models with **lower** AIC and BIC values are considered better.\n",
        "\n",
        "*Note:* For models with the same number of predictors, $R^2_{adj}$, AIC, and BIC will all select the exact same subset. They only differ when comparing models with different numbers of predictors.\n"
      ],
      "metadata": {
        "id": "SmmoLk_xowtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.5. EXHAUSTIVE SEARCH FOR REDUCING PREDIICTORS IN TOYOTA COROLLA EXAMPLE**"
      ],
      "metadata": {
        "id": "z0CodUlfTxL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code exhaustive, adjusted_r2_score, AIC_score\n",
        "\n",
        "# exhaustive search\n",
        "import itertools\n",
        "\n",
        "def exhausted_search(variables, train_model, score_model):\n",
        "  \"\"\"Variable selection using backward elimination\n",
        "\n",
        "  Input:\n",
        "    variables: complete list of variables to consider in model building\n",
        "    train_model: function that returns a fitted model for a given set of variables\n",
        "    score_model: function that returns the score of a model; better models have lower scores\n",
        "\n",
        "  Returns:\n",
        "    List of best subset models for increasing number of variables\n",
        "  \"\"\"\n",
        "\n",
        "  # create models of increasing size and determine the best models in each case\n",
        "  result = []\n",
        "  for nvariables in range (1, len(variables)+ 1):\n",
        "    best_subset = None\n",
        "    best_score = None\n",
        "    best_model = None\n",
        "    for subset in itertools.combinations(variables, nvariables):\n",
        "        subset = list(subset)\n",
        "        subset_model = train_model(subset)\n",
        "        subset_score = score_model(subset_model, subset)\n",
        "        if best_subset is None or best_score > subset_score:\n",
        "              best_subset = subset\n",
        "              best_score = subset_score\n",
        "              best_model = subset_model\n",
        "    result.append({\n",
        "        'n' : nvariables,\n",
        "        'variables': best_subset,\n",
        "        'score': best_score,\n",
        "        'model': best_model,\n",
        "    })\n",
        "\n",
        "  return result\n",
        "\n",
        "# adjusted_r2_score\n",
        "from sklearn.metrics import r2_score\n",
        "def adjusted_r2_score(y_true, y_pred, model):\n",
        "  \"\"\"calculate adjusted R2\n",
        "  Input:\n",
        "      y_true : actual value\n",
        "      y_pred: predicted value\n",
        "      model: predictive model\n",
        "   \"\"\"\n",
        "  n = len(y_pred)\n",
        "  p = len(model.coef_)\n",
        "  r2 = r2_score(y_true, y_pred)\n",
        "  return 1 - (1 -r2) * (n-1)/ (n - p - 1)\n",
        "\n",
        "# AIC_score\n",
        "import math\n",
        "import numpy as np\n",
        "def AIC_score(y_true, y_pred, model = None, df = None):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "        y_true: actual values\n",
        "        y_pred: predicted values\n",
        "        model (optional): predictive model\n",
        "        df (optional): degrees of freedom of model\n",
        "\n",
        "  One of model or df is required\n",
        "  \"\"\"\n",
        "\n",
        "  if df is None and model is None:\n",
        "    raise ValueError('You need to provide either model or df')\n",
        "  n = len(y_pred)\n",
        "  p = len(model.coef_) + 1 if df is None else df\n",
        "  resid = np.array(y_true) - np.array(y_pred)\n",
        "  sse = np.sum(resid ** 2)\n",
        "  constant = n + n * np.log(2*np.pi)\n",
        "  return n*math.log(sse/n) + constant + 2 * (p + 1)\n",
        "\n",
        "# code for 6.5\n",
        "def train_model(variables):\n",
        "  model = LinearRegression()\n",
        "  model.fit(train_X[list(variables)], train_y)\n",
        "  return model\n",
        "\n",
        "def score_model(model, variables):\n",
        "  pred_y = model.predict(train_X[list(variables)])\n",
        "  # we negate as score is optimized to be as low as possible\n",
        "  return -adjusted_r2_score(train_y, pred_y, model)\n",
        "\n",
        "allVariables = train_X.columns\n",
        "results = exhausted_search(allVariables, train_model, score_model)\n",
        "\n",
        "data = []\n",
        "for result in results:\n",
        "  model = result['model']\n",
        "  variables = list(result['variables'])\n",
        "  AIC = AIC_score(train_y, model.predict(train_X[variables]),  model)\n",
        "\n",
        "  d = {'n' : result['n'], 'r2adj': -result['score'], 'AIC': AIC}\n",
        "  d.update({var: var in result['variables'] for var in allVariables})\n",
        "  data.append(d)\n",
        "\n",
        "# Construct the list of all column names correctly\n",
        "alldf_columns = list(('n', 'r2adj', 'AIC')) + list(allVariables)\n",
        "pd.DataFrame(data, columns = alldf_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "LqK8c7R1zlh0",
        "outputId": "9f018bd0-ce10-4a57-f33c-ae8b7c9f7b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     n     r2adj           AIC  Age_08_04     KM     HP  Met_Color  Automatic  \\\n",
              "0    1  0.767901  10689.712094       True  False  False      False      False   \n",
              "1    2  0.801160  10597.910645       True  False   True      False      False   \n",
              "2    3  0.829659  10506.084235       True  False   True      False      False   \n",
              "3    4  0.846357  10445.174820       True   True   True      False      False   \n",
              "4    5  0.849044  10435.578836       True   True   True      False      False   \n",
              "5    6  0.853172  10419.932278       True   True   True      False      False   \n",
              "6    7  0.853860  10418.104025       True   True   True      False      False   \n",
              "7    8  0.854297  10417.290103       True   True   True      False       True   \n",
              "8    9  0.854172  10418.789079       True   True   True      False       True   \n",
              "9   10  0.854036  10420.330800       True   True   True       True       True   \n",
              "10  11  0.853796  10422.298278       True   True   True       True       True   \n",
              "\n",
              "       CC  Doors  Quarterly_Tax  Weight  Fuel_Type_Diesel  Fuel_Type_Petrol  \n",
              "0   False  False          False   False             False             False  \n",
              "1   False  False          False   False             False             False  \n",
              "2   False  False          False    True             False             False  \n",
              "3   False  False          False    True             False             False  \n",
              "4   False  False           True    True             False             False  \n",
              "5   False  False           True    True             False              True  \n",
              "6   False  False           True    True              True              True  \n",
              "7   False  False           True    True              True              True  \n",
              "8   False   True           True    True              True              True  \n",
              "9   False   True           True    True              True              True  \n",
              "10   True   True           True    True              True              True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39ab02b5-5125-4378-afba-648e61ab4f49\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>r2adj</th>\n",
              "      <th>AIC</th>\n",
              "      <th>Age_08_04</th>\n",
              "      <th>KM</th>\n",
              "      <th>HP</th>\n",
              "      <th>Met_Color</th>\n",
              "      <th>Automatic</th>\n",
              "      <th>CC</th>\n",
              "      <th>Doors</th>\n",
              "      <th>Quarterly_Tax</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Fuel_Type_Diesel</th>\n",
              "      <th>Fuel_Type_Petrol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.767901</td>\n",
              "      <td>10689.712094</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.801160</td>\n",
              "      <td>10597.910645</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.829659</td>\n",
              "      <td>10506.084235</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.846357</td>\n",
              "      <td>10445.174820</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.849044</td>\n",
              "      <td>10435.578836</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.853172</td>\n",
              "      <td>10419.932278</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.853860</td>\n",
              "      <td>10418.104025</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.854297</td>\n",
              "      <td>10417.290103</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.854172</td>\n",
              "      <td>10418.789079</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.854036</td>\n",
              "      <td>10420.330800</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.853796</td>\n",
              "      <td>10422.298278</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39ab02b5-5125-4378-afba-648e61ab4f49')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-39ab02b5-5125-4378-afba-648e61ab4f49 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-39ab02b5-5125-4378-afba-648e61ab4f49');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-44ba62b5-c5ab-4f07-80b1-84a1f92ed216\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44ba62b5-c5ab-4f07-80b1-84a1f92ed216')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-44ba62b5-c5ab-4f07-80b1-84a1f92ed216 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 11,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6,\n          1,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r2adj\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02836888471113058,\n        \"min\": 0.7679012055663247,\n        \"max\": 0.8542973546056307,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8531723015240328,\n          0.7679012055663247,\n          0.8540361679814439\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AIC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 91.12638580528294,\n        \"min\": 10417.29010273851,\n        \"max\": 10689.712094434577,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          10419.93227806251,\n          10689.712094434577,\n          10420.330799866977\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age_08_04\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KM\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HP\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Met_Color\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Automatic\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CC\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Doors\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quarterly_Tax\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel_Type_Diesel\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fuel_Type_Petrol\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: Popular Subset Selection Algorithms (Iterative Search)\n",
        "\n",
        "When the number of predictors is large, an exhaustive search is computationally infeasible. In such cases, we use iterative search methods:\n",
        "\n",
        "**1. Forward Selection:**\n",
        "* Start with no predictors (null model). Add predictors one by one.\n",
        "* At each step, add the predictor that makes the largest contribution to $R^2$.\n",
        "* Stop when the contribution of additional predictors is not statistically significant.\n",
        "* *Downside:* It may miss pairs or groups of predictors that work very well together but perform poorly as single predictors.\n",
        "\n",
        "**2. Backward Elimination:**\n",
        "* Start with all predictors (full model).\n",
        "* At each step, remove the least useful predictor (the one with the least statistical significance).\n",
        "* Stop when all remaining predictors contribute significantly.\n",
        "\n",
        "**3. Stepwise Regression:**\n",
        "* A flexible combination of both methods: at each step, the algorithm considers adding a new variable or removing an existing one, depending on how the evaluation criteria change.\n"
      ],
      "metadata": {
        "id": "q2-h6MIQpERa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.6. BACKWARD ELIMINATION FOR REDUCING PREDICTORS IN TOYOTA COROLLA EXAMPLE**"
      ],
      "metadata": {
        "id": "FXMGw50fUA1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backward_elimination\n",
        "\n"
      ],
      "metadata": {
        "id": "pvvN4R0V45l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.7. FORWARD SELECTION FOR REDUCING PREDICTORS IN TOYOTA COROLLA EXAMPLE**"
      ],
      "metadata": {
        "id": "aq7kj3sBUKMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward elimination\n",
        "\n"
      ],
      "metadata": {
        "id": "Jdm2N3ohukYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.8 STEPWISE REGRESSION FOR REDUCING PREDICTORS IN TOYOTA COROLLA EXAMPLE**"
      ],
      "metadata": {
        "id": "ZGr16_8TUSk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REGULARIZATION (SHRINKAGE MODELS)**"
      ],
      "metadata": {
        "id": "EeYp_RE9UaXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Concept: Why Regularization?\n",
        "* **The Problem:** In standard Linear Regression, if predictors are highly correlated (**multicollinearity**), the estimates of coefficients become unstable. They exhibit high **standard errors** (high variance), meaning small changes in training data can lead to drastically different models.\n",
        "* **The Solution (Shrinkage):** Instead of just selecting a subset of variables (setting coefficients to 0), **Regularization** shrinks the coefficients *towards* 0.\n",
        "* **The Trade-off:** By constraining the total magnitude of coefficients, we accept a slight increase in **Bias** to achieve a significant reduction in **Variance**, leading to better prediction performance on new data.\n",
        "\n",
        "### 2. Key Methods: Ridge vs. Lasso\n",
        "\n",
        "#### a. Ridge Regression (L2 Penalty)\n",
        "* **Mechanism:** Adds a penalty term based on the **sum of squared coefficients** ($\\sum \\beta^2$).\n",
        "* **Effect:** Shrinks coefficients towards zero but rarely makes them exactly zero.\n",
        "* **Use Case:** Good when you want to keep all variables but reduce their impact/instability.\n",
        "\n",
        "#### b. Lasso Regression (L1 Penalty)\n",
        "* **Mechanism:** Adds a penalty term based on the **sum of absolute values of coefficients** ($\\sum |\\beta|$).\n",
        "* **Effect:** Can shrink coefficients **exactly to zero**.\n",
        "* **Use Case:** Functions as both a regression model and a **Feature Selection** tool (it automatically discards irrelevant variables).\n"
      ],
      "metadata": {
        "id": "_idoPLnCpUU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Python Implementation (Scikit-learn)\n",
        "\n",
        "Implementation is done via `sklearn.linear_model`.\n",
        "\n",
        "* **Classes:** Use `Ridge` and `Lasso`.\n",
        "* **The Alpha Parameter ($\\alpha$):**\n",
        "    * Controls the strength of the penalty.\n",
        "    * `alpha = 0`: Equivalent to standard Linear Regression (OLS).\n",
        "    * Higher `alpha`: Stronger regularization (coefficients shrink more).\n",
        "* **Automatic Parameter Selection (Auto-tuning):**\n",
        "    * `RidgeCV` and `LassoCV`: Use **Cross-Validation** to find the optimal $\\alpha$.\n",
        "    * `BayesianRidge`: Uses an iterative probability-based approach to infer parameters from the data.\n",
        "* **Crucial Pre-processing Step:**\n",
        "    * **Standardization is mandatory.** Since penalties are based on the magnitude of coefficients, all predictors must be on the same scale. Use `StandardScaler` or ensure `normalize=True`.\n",
        "\n",
        "### 4. Alternative Implementation (Statsmodels)\n",
        "\n",
        "While `scikit-learn` is optimized for prediction, `statsmodels` is better for **statistical inference** (p-values, confidence intervals).\n",
        "\n",
        "* **Method:** Use `sm.OLS`.\n",
        "* **Function:** `OLS.fit_regularized`.\n",
        "* **Parameters:**\n",
        "    * `L1_wt = 0`: Ridge Regression.\n",
        "    * `L1_wt = 1`: Lasso Regression.\n",
        "    * `0 < L1_wt < 1`: Elastic Net (combination of both).\n"
      ],
      "metadata": {
        "id": "y_YRRRT2pdeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.9. LASSO AND RIDGE REGRESSION APPLIED TO THE TOYOTA COROLLA DATA**"
      ],
      "metadata": {
        "id": "MZRjVpo1UgbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE 6.10. LINEAR REGRESSION MODEL OF PRICE VS. CAR ATTRIBUTES USING STATSMODELS (COMPARE WITH TABLE 6.3)**"
      ],
      "metadata": {
        "id": "WpLNyBmmUqiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPENDIX: USING STATMODELS"
      ],
      "metadata": {
        "id": "Hugi2j4RU6Na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While `scikit-learn` is optimized for prediction, `statsmodels` is better for **statistical inference** (p-values, confidence intervals).\n",
        "\n",
        "* **Method:** Use `sm.OLS`.\n",
        "* **Function:** `OLS.fit_regularized`.\n",
        "* **Parameters:**\n",
        "    * `L1_wt = 0`: Ridge Regression.\n",
        "    * `L1_wt = 1`: Lasso Regression.\n",
        "    * `0 < L1_wt < 1`: Elastic Net (combination of both).\n"
      ],
      "metadata": {
        "id": "yvrG-1YypkPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Parsimony Principle:** If a regularized model (like Lasso) selects fewer variables but performs similarly to a complex model, prefer the simpler one. Simple models are easier to interpret and maintain.\n",
        "\n",
        "> **Data Context:** Regularization shines when you have **many variables** with **high correlation**. In datasets with few, uncorrelated variables (like the specific Toyota Corolla example), standard OLS might perform just as well or better because the bias introduced by regularization outweighs the variance reduction.\n"
      ],
      "metadata": {
        "id": "NmBMMrYuSlFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "\n",
        "* [Kitchen Sink Model](https://www.vexpower.com/brief/kitchen-sink-model#:~:text=In%20data%20science%2C%20a%20kitchen,everything%20but%20the%20kitchen%20sink.)\n",
        "\n",
        "* [Sparsity and Parsimonious Models: Everything should be made as simple as possible, but no simpler](https://www.youtube.com/watch?v=9eGMJ3-wmm0&embeds_referring_euri=https%3A%2F%2Fwww.google.com%2Fsearch%3Fq%3Dwhat%2Bis%2Bparsimony%2Bin%2Bmodelling%26sca_esv%3D433c76685208b106%26udm%3D7%26biw%3D711%26bih%3D633%26sxsrf%3DAE3TifPmXzKQ&source_ve_path=MTY0OTksMjM4NTE)\n",
        "\n",
        "* [Rules of Thumb in Data Engineering](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ms_tr_99_100_rules_of_thumb_in_data_engineering.pdf)\n",
        "\n",
        "* [Machine Learning Fundamentals: Bias and Variance](https://www.youtube.com/watch?v=EuBBz3bI-aA)\n",
        "\n",
        "* [Statistics 101: Multiple Regression, Forward Selection](https://www.youtube.com/watch?v=0UJcGPR2W5U)\n",
        "\n",
        "* [Statistics 101: Multiple Regression, Backward Elimination](https://www.youtube.com/watch?v=pv4SBxyynxc)\n",
        "\n",
        "* [Statistics 101: Multiple Regression, Stepwise Regression](https://www.youtube.com/watch?v=An40g_j1dHA)\n",
        "\n",
        "* [Shrinkage methods](https://web.stanford.edu/class/stats202/notes/Model-selection/Shrinkage.html)"
      ],
      "metadata": {
        "id": "pUT5CiwEgJvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$NOTES:$"
      ],
      "metadata": {
        "id": "aDYGapTYT8OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 MODEL BUILDING TECHNIQUES\n",
        "\n",
        "To build an effective linear regression model, a Data Scientist must balance minimizing error with optimizing model complexity. Below is a systematic summary ranging from core metrics to execution processes.\n",
        "\n",
        "---\n",
        "\n",
        "## PART 1: MATHEMATICAL FOUNDATIONS & DECISION METRICS\n",
        "\n",
        "Understanding these formulas is key to grasping the \"rules of the game\" regarding adding or removing variables in a model.\n",
        "\n",
        "### 1. Squared Semi-partial Correlation\n",
        "$$sr_i^2 = R^2_{model\\_with\\_Xi} - R^2_{model\\_without\\_Xi}$$\n",
        "* **Interpretation:** **Unique Incremental Value.** Measures the amount of variance in $Y$ that variable $X_i$ explains *independently*, after removing the overlap with other variables. This is the gold standard for selecting variables in Stepwise/Forward techniques.\n",
        "\n",
        "### 2. SSE (Sum of Squares Error)\n",
        "$$SSE = \\sum (y - \\hat{y})^2$$\n",
        "* **Interpretation:** **Ultimate Goal.** The total sum of squared errors. All algorithms aim to minimize SSE. A lower SSE means the model fits the data better.\n",
        "\n",
        "### 3. SST (Total Sum of Squares)\n",
        "$$SST = \\sum (y - \\bar{y})^2$$\n",
        "* **Interpretation:** **Total Variance.** Represents the total variability inherent in the actual data before any regression analysis. It is the baseline for calculating $R^2$.\n",
        "\n",
        "### 4. $R^2$ (Coefficient of Determination)\n",
        "$$R^2 = 1 - \\frac{SSE}{SST}$$\n",
        "* **Interpretation:** The proportion of variance in $Y$ explained by the model. $R^2$ always increases when a variable is added, regardless of the variable's significance.\n",
        "\n",
        "### 5. $R^2_{adj}$ (Adjusted R-squared)\n",
        "$$R^2_{adj} = 1 - (1-R^2)\\frac{n-1}{n-k-1}$$\n",
        "* **Interpretation:** **Penalized Accuracy.** Overcomes the flaw of $R^2$. It only increases if the new variable improves the model *more than* would be expected by chance. It decreases if \"junk\" variables are added.\n",
        "\n",
        "### 6. Df Error (Degrees of Freedom)\n",
        "$$df_{error} = n - k - 1$$\n",
        "* **Interpretation:** **The Cost.** Every time a variable is added ($k$ increases), we lose 1 degree of freedom used to estimate error. If too many $df$ are lost (complex model) without a significant drop in SSE, the model weakens.\n",
        "\n",
        "### 7. RMSE (Root Mean Square Error)\n",
        "$$RMSE = \\sqrt{\\frac{SSE}{df_{error}}}$$\n",
        "* **Interpretation:** The standard deviation of the residuals (prediction errors). Used to compare the actual accuracy between models with different numbers of variables.\n",
        "\n",
        "### 8. F-Ratio\n",
        "$$F = \\frac{MS_{regression}}{MS_{error}}$$\n",
        "* **Interpretation:** **Basis for Stopping Rule (P-value).** Compares the explained variance to the unexplained (error) variance. A large F (corresponding to a small p-value) indicates that the reduction in SSE is statistically significant, not just due to luck.\n",
        "\n",
        "---\n",
        "\n",
        "## PART 2: MODEL BUILDING PROCESSES\n",
        "\n",
        "There are 4 main techniques for variable selection, divided into 2 groups: **Iterative** and **Non-iterative**.\n",
        "\n",
        "### I. Forward Selection (Iterative)\n",
        "*Process goes from Simple to Complex.*\n",
        "\n",
        "1.  **Process:**\n",
        "    * **Initialization:** Start with a **Null Model** (intercept only, no variables).\n",
        "    * **Loop:**\n",
        "        * Examine candidate variables one by one.\n",
        "        * In the first step, select the variable with the highest correlation ($r$) with $Y$.\n",
        "        * In subsequent steps, select the variable with the highest **Squared Semi-partial Correlation ($sr^2$)**. This is the variable that reduces SSE the most on the residuals left unexplained by previous variables.\n",
        "    * **Stopping Rule:** The selected variable must pass a \"hurdle\" (typically **p-value < 0.05**). If the reduction in SSE is not statistically significant, the process stops.\n",
        "2.  **Core Characteristic:** \"Once in, never out.\" Once a variable enters the model, it stays forever.\n",
        "3.  **Flaws:**\n",
        "    * **Redundancy:** A variable entered early may become insignificant when stronger variables enter later (due to multicollinearity or overlap), but Forward Selection has no mechanism to remove it.\n",
        "\n",
        "### II. Backward Elimination (Iterative)\n",
        "*Process goes from Complex to Simple.*\n",
        "\n",
        "1.  **Process:**\n",
        "    * **Initialization:** Start with a **Full Model** (containing all variables).\n",
        "    * **Loop:**\n",
        "        * Test the significance of all current variables.\n",
        "        * Identify the variable contributing the least to error reduction (highest p-value).\n",
        "        * If this variable's p-value violates the stopping rule (e.g., > 0.05), it is removed.\n",
        "    * **Conclusion:** Repeat until all remaining variables are statistically significant.\n",
        "2.  **Core Characteristic:** \"Once out, never back.\" Once a variable is removed, it is gone forever.\n",
        "3.  **Flaws:**\n",
        "    * **Lost Potential:** A variable removed early might have been important if paired with other variables later (interaction effects), but it loses the chance to return.\n",
        "    * **Temptation Factor:** Analysts may be tempted to keep too many variables if the stopping rule is not strictly defined.\n",
        "\n",
        "### III. Stepwise Regression (Iterative)\n",
        "*A smart hybrid technique overcoming the flaws of the previous two.*\n",
        "\n",
        "1.  **Process:**\n",
        "    * **Add Step (Forward):** Start like Forward Selection, adding the variable with the highest $sr^2$.\n",
        "    * **Check Step (Backward):** Immediately after adding a new variable, the algorithm performs a \"backward look.\" It re-evaluates all old variables in the model.\n",
        "    * **Removal:** If the arrival of the new variable makes an old variable redundant (its p-value increases and loses significance), the old variable is removed.\n",
        "2.  **Pros:**\n",
        "    * **Flexible:** Variables can enter and leave the model.\n",
        "    * **Handles Multicollinearity:** Effectively manages overlapping information between variables.\n",
        "\n",
        "### IV. Best Subsets Regression (Non-iterative)\n",
        "*The \"Brute-force\" technique.*\n",
        "\n",
        "1.  **Process:**\n",
        "    * The computer tests **all possible combinations** of variables.\n",
        "    * It runs models with 1 variable, 2 variables, ..., up to $k$ variables.\n",
        "    * It identifies the absolute best model (lowest SSE, highest $R^2_{adj}$) among thousands of combinations.\n",
        "2.  **The Trade-off:**\n",
        "    * **Pro:** Guarantees finding the mathematically optimal model.\n",
        "    * **Con:** **Computationally expensive**. The number of models to run is $2^k$, which increases exponentially with the number of variables.\n",
        "\n",
        "---\n",
        "\n",
        "## PART 3: ADVICE & CONCLUSION\n",
        "\n",
        "1.  **No \"Single Truth\":**\n",
        "    * On the same dataset, the 4 methods (Forward, Backward, Stepwise, Best Subsets) may propose **4 different \"best\" models**. None are absolutely wrong; they simply optimize via different mathematical paths.\n",
        "\n",
        "2.  **The Decisive Role of the Data Scientist:**\n",
        "    * Computers only run algorithms. Humans decide the outcome:\n",
        "        * **Stopping Rule:** Do you choose a strict p-value (0.01) or a loose one (0.10)?\n",
        "        * **Strategy:** Do you choose Best Subsets (for small data) or Stepwise (for big data)?\n",
        "        * **Business Sense:** A variable might be statistically significant (good p-value) but nonsensical in business terms (e.g., house price increases as size decreases). In such cases, the Data Scientist must remove it regardless of the algorithm."
      ],
      "metadata": {
        "id": "OpZALU_rUhwF"
      }
    }
  ]
}